<!DOCTYPE html>
<!-- saved from url=(0027)http://localhost:9090/rules -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
        <meta name="robots" content="noindex,nofollow">
        <title>Prometheus Time Series Collection and Processing Server</title>
        <link rel="shortcut icon" href="http://localhost:9090/static/img/favicon.ico?v=a6600f564e3c483cc820bae6c7a551db701a22b3">
        <script src="./Prometheus Time Series Collection and Processing Server_files/jquery-3.3.1.min.js"></script>    
        <script src="./Prometheus Time Series Collection and Processing Server_files/popper.min.js"></script>
        <script src="./Prometheus Time Series Collection and Processing Server_files/bootstrap.min.js"></script>

        <link type="text/css" rel="stylesheet" href="./Prometheus Time Series Collection and Processing Server_files/bootstrap.min.css">
        <link type="text/css" rel="stylesheet" href="./Prometheus Time Series Collection and Processing Server_files/prometheus.css">
        <link type="text/css" rel="stylesheet" href="./Prometheus Time Series Collection and Processing Server_files/bootstrap-glyphicons.min.css">

        <script>
            var PATH_PREFIX = "";
            var BUILD_VERSION = "a6600f564e3c483cc820bae6c7a551db701a22b3";
            $(function () {
                $('[data-toggle="tooltip"]').tooltip()
            })
        </script>

        
<link type="text/css" rel="stylesheet" href="./Prometheus Time Series Collection and Processing Server_files/rules.css">

    </head>

    <body>
        <nav class="navbar fixed-top navbar-expand-sm navbar-dark bg-dark">
            <div class="container-fluid">      

                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#nav-content" aria-expanded="false" aria-controls="nav-content" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                    
                </button>

                <a class="navbar-brand" href="http://localhost:9090/">Prometheus</a>


                <div id="nav-content" class="navbar-collapse collapse">
                    <ul class="navbar-nav">
                        
                        
                        <li class="nav-item"><a class="nav-link" href="http://localhost:9090/alerts">Alerts</a></li>
                        <li class="nav-item"><a class="nav-link" href="http://localhost:9090/graph">Graph</a></li>
                        <li class="nav-item dropdown">
                            <a href="http://localhost:9090/rules#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Status <span class="caret"></span></a>
                            <div class="dropdown-menu">
                                <a class="dropdown-item" href="http://localhost:9090/status">Runtime &amp; Build Information</a>
                                <a class="dropdown-item" href="http://localhost:9090/flags">Command-Line Flags</a>
                                <a class="dropdown-item" href="http://localhost:9090/config">Configuration</a>
                                <a class="dropdown-item" href="http://localhost:9090/rules">Rules</a>
                                <a class="dropdown-item" href="http://localhost:9090/targets">Targets</a>
                                <a class="dropdown-item" href="http://localhost:9090/service-discovery">Service Discovery</a>
                            </div>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://prometheus.io/docs/prometheus/latest/getting_started/" target="_blank">Help</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        
  <div class="container-fluid">
    <h2>Rules</h2>
    <table class="table table-bordered">
      
        <thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#alertmanager.rules" id="alertmanager.rules">alertmanager.rules</a></h2></td>
            <td><h2>12.521s ago</h2></td>
            <td><h2>617.6us</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22AlertmanagerConfigInconsistent%22%7D&amp;g0.tab=1">AlertmanagerConfigInconsistent</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+by%28namespace%2C+service%29+%28count_values+by%28namespace%2C+service%29+%28%22config_hash%22%2C+alertmanager_config_hash%7Bjob%3D%22prometheus-operator-alertmanager%22%2Cnamespace%3D%22monitoring%22%7D%29%29+%21%3D+1&amp;g0.tab=1">count
  by(namespace, service) (count_values by(namespace, service) ("config_hash",
  alertmanager_config_hash{job="prometheus-operator-alertmanager",namespace="monitoring"}))
  != 1</a>
for: 5m
labels:
  severity: critical
annotations:
  message: |
    The configuration of the instances of the Alertmanager cluster `{{ $labels.namespace }}/{{ $labels.service }}` are out of sync.
    {{ range printf "alertmanager_config_hash{namespace=\"%s\",service=\"%s\"}" $labels.namespace $labels.service | query }}
    Configuration hash for pod {{ .Labels.pod }} is "{{ printf "%.f" .Value }}"
    {{ end }}
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              12.521s ago
            </td>
            <td>401.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22AlertmanagerFailedReload%22%7D&amp;g0.tab=1">AlertmanagerFailedReload</a>
expr: <a href="http://localhost:9090/graph?g0.expr=alertmanager_config_last_reload_successful%7Bjob%3D%22prometheus-operator-alertmanager%22%2Cnamespace%3D%22monitoring%22%7D+%3D%3D+0&amp;g0.tab=1">alertmanager_config_last_reload_successful{job="prometheus-operator-alertmanager",namespace="monitoring"}
  == 0</a>
for: 10m
labels:
  severity: warning
annotations:
  message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
    }}/{{ $labels.pod}}.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              12.521s ago
            </td>
            <td>88.51us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22AlertmanagerMembersInconsistent%22%7D&amp;g0.tab=1">AlertmanagerMembersInconsistent</a>
expr: <a href="http://localhost:9090/graph?g0.expr=alertmanager_cluster_members%7Bjob%3D%22prometheus-operator-alertmanager%22%2Cnamespace%3D%22monitoring%22%7D+%21%3D+on%28service%29+group_left%28%29+count+by%28service%29+%28alertmanager_cluster_members%7Bjob%3D%22prometheus-operator-alertmanager%22%2Cnamespace%3D%22monitoring%22%7D%29&amp;g0.tab=1">alertmanager_cluster_members{job="prometheus-operator-alertmanager",namespace="monitoring"}
  != on(service) group_left() count by(service) (alertmanager_cluster_members{job="prometheus-operator-alertmanager",namespace="monitoring"})</a>
for: 5m
labels:
  severity: critical
annotations:
  message: Alertmanager has not found all other members of the cluster.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              12.521s ago
            </td>
            <td>113.8us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#etcd" id="etcd">etcd</a></h2></td>
            <td><h2>18.36s ago</h2></td>
            <td><h2>2.206ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdMembersDown%22%7D&amp;g0.tab=1">etcdMembersDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0&amp;g0.tab=1">max
  by(job) (sum by(job) (up{job=~".*etcd.*"} == bool 0) or count by(job, endpoint)
  (sum by(job, endpoint, To) (rate(etcd_network_peer_sent_failures_total{job=~".*etcd.*"}[3m]))
  &gt; 0.01)) &gt; 0</a>
for: 3m
labels:
  severity: critical
annotations:
  message: 'etcd cluster "{{ $labels.job }}": members are down ({{ $value
    }}).'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>480.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdInsufficientMembers%22%7D&amp;g0.tab=1">etcdInsufficientMembers</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+1%29+%3C+%28%28count+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%29+%2B+1%29+%2F+2%29&amp;g0.tab=1">sum
  by(job) (up{job=~".*etcd.*"} == bool 1) &lt; ((count by(job) (up{job=~".*etcd.*"})
  + 1) / 2)</a>
for: 3m
labels:
  severity: critical
annotations:
  message: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
    }}).'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>178.9us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdNoLeader%22%7D&amp;g0.tab=1">etcdNoLeader</a>
expr: <a href="http://localhost:9090/graph?g0.expr=etcd_server_has_leader%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+0&amp;g0.tab=1">etcd_server_has_leader{job=~".*etcd.*"}
  == 0</a>
for: 1m
labels:
  severity: critical
annotations:
  message: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }}
    has no leader.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>102.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfLeaderChanges%22%7D&amp;g0.tab=1">etcdHighNumberOfLeaderChanges</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28%28max+by%28job%29+%28etcd_server_leader_changes_seen_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%29+or+0+%2A+absent%28etcd_server_leader_changes_seen_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%29%29%5B15m%3A1m%5D%29+%3E%3D+3&amp;g0.tab=1">increase((max
  by(job) (etcd_server_leader_changes_seen_total{job=~".*etcd.*"}) or 0 *
  absent(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}))[15m:1m])
  &gt;= 3</a>
for: 5m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": {{ $value }} leader changes
    within the last 15 minutes. Frequent elections may be a sign of insufficient resources,
    high network latency, or disruptions by other components and should be investigated.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>242.2us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfFailedGRPCRequests%22%7D&amp;g0.tab=1">etcdHighNumberOfFailedGRPCRequests</a>
expr: <a href="http://localhost:9090/graph?g0.expr=100+%2A+sum+by%28job%2C+instance%2C+grpc_service%2C+grpc_method%29+%28rate%28grpc_server_handled_total%7Bgrpc_code%21%3D%22OK%22%2Cjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%2F+sum+by%28job%2C+instance%2C+grpc_service%2C+grpc_method%29+%28rate%28grpc_server_handled_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+1&amp;g0.tab=1">100
  * sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{grpc_code!="OK",job=~".*etcd.*"}[5m]))
  / sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{job=~".*etcd.*"}[5m]))
  &gt; 1</a>
for: 10m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
    {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>191us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfFailedGRPCRequests%22%7D&amp;g0.tab=1">etcdHighNumberOfFailedGRPCRequests</a>
expr: <a href="http://localhost:9090/graph?g0.expr=100+%2A+sum+by%28job%2C+instance%2C+grpc_service%2C+grpc_method%29+%28rate%28grpc_server_handled_total%7Bgrpc_code%21%3D%22OK%22%2Cjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%2F+sum+by%28job%2C+instance%2C+grpc_service%2C+grpc_method%29+%28rate%28grpc_server_handled_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+5&amp;g0.tab=1">100
  * sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{grpc_code!="OK",job=~".*etcd.*"}[5m]))
  / sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{job=~".*etcd.*"}[5m]))
  &gt; 5</a>
for: 5m
labels:
  severity: critical
annotations:
  message: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
    {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>170.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdGRPCRequestsSlow%22%7D&amp;g0.tab=1">etcdGRPCRequestsSlow</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+by%28job%2C+instance%2C+grpc_service%2C+grpc_method%2C+le%29+%28rate%28grpc_server_handling_seconds_bucket%7Bgrpc_type%3D%22unary%22%2Cjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29%29+%3E+0.15&amp;g0.tab=1">histogram_quantile(0.99,
  sum by(job, instance, grpc_service, grpc_method, le) (rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job=~".*etcd.*"}[5m])))
  &gt; 0.15</a>
for: 10m
labels:
  severity: critical
annotations:
  message: 'etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method
    }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>114.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdMemberCommunicationSlow%22%7D&amp;g0.tab=1">etcdMemberCommunicationSlow</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+rate%28etcd_network_peer_round_trip_time_seconds_bucket%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+0.15&amp;g0.tab=1">histogram_quantile(0.99,
  rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
  &gt; 0.15</a>
for: 10m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": member communication with {{
    $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>91.31us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfFailedProposals%22%7D&amp;g0.tab=1">etcdHighNumberOfFailedProposals</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28etcd_server_proposals_failed_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B15m%5D%29+%3E+5&amp;g0.tab=1">rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m])
  &gt; 5</a>
for: 15m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures
    within the last 30 minutes on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>77.7us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighFsyncDurations%22%7D&amp;g0.tab=1">etcdHighFsyncDurations</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+rate%28etcd_disk_wal_fsync_duration_seconds_bucket%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+0.5&amp;g0.tab=1">histogram_quantile(0.99,
  rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
  &gt; 0.5</a>
for: 10m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": 99th percentile fync durations
    are {{ $value }}s on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>84.21us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighCommitDurations%22%7D&amp;g0.tab=1">etcdHighCommitDurations</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+rate%28etcd_disk_backend_commit_duration_seconds_bucket%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+0.25&amp;g0.tab=1">histogram_quantile(0.99,
  rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
  &gt; 0.25</a>
for: 10m
labels:
  severity: warning
annotations:
  message: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
    {{ $value }}s on etcd instance {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>90.41us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfFailedHTTPRequests%22%7D&amp;g0.tab=1">etcdHighNumberOfFailedHTTPRequests</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28method%29+%28rate%28etcd_http_failed_total%7Bcode%21%3D%22404%22%2Cjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%2F+sum+by%28method%29+%28rate%28etcd_http_received_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+0.01&amp;g0.tab=1">sum
  by(method) (rate(etcd_http_failed_total{code!="404",job=~".*etcd.*"}[5m]))
  / sum by(method) (rate(etcd_http_received_total{job=~".*etcd.*"}[5m])) &gt;
  0.01</a>
for: 10m
labels:
  severity: warning
annotations:
  message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance
    {{ $labels.instance }}'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>150.9us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHighNumberOfFailedHTTPRequests%22%7D&amp;g0.tab=1">etcdHighNumberOfFailedHTTPRequests</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28method%29+%28rate%28etcd_http_failed_total%7Bcode%21%3D%22404%22%2Cjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%2F+sum+by%28method%29+%28rate%28etcd_http_received_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B5m%5D%29%29+%3E+0.05&amp;g0.tab=1">sum
  by(method) (rate(etcd_http_failed_total{code!="404",job=~".*etcd.*"}[5m]))
  / sum by(method) (rate(etcd_http_received_total{job=~".*etcd.*"}[5m])) &gt;
  0.05</a>
for: 10m
labels:
  severity: critical
annotations:
  message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance
    {{ $labels.instance }}.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>147.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22etcdHTTPRequestsSlow%22%7D&amp;g0.tab=1">etcdHTTPRequestsSlow</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+rate%28etcd_http_successful_duration_seconds_bucket%5B5m%5D%29%29+%3E+0.15&amp;g0.tab=1">histogram_quantile(0.99,
  rate(etcd_http_successful_duration_seconds_bucket[5m])) &gt; 0.15</a>
for: 10m
labels:
  severity: warning
annotations:
  message: etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
    }} are slow.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.36s ago
            </td>
            <td>66.9us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#general.rules" id="general.rules">general.rules</a></h2></td>
            <td><h2>6.798s ago</h2></td>
            <td><h2>3.239ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22TargetDown%22%7D&amp;g0.tab=1">TargetDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=100+%2A+%28count+by%28job%2C+namespace%2C+service%29+%28up+%3D%3D+0%29+%2F+count+by%28job%2C+namespace%2C+service%29+%28up%29%29+%3E+10&amp;g0.tab=1">100
  * (count by(job, namespace, service) (up == 0) / count by(job, namespace, service)
  (up)) &gt; 10</a>
for: 10m
labels:
  severity: warning
annotations:
  message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
    }} targets in {{ $labels.namespace }} namespace are down.'
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.798s ago
            </td>
            <td>2.956ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22Watchdog%22%7D&amp;g0.tab=1">Watchdog</a>
expr: <a href="http://localhost:9090/graph?g0.expr=vector%281%29&amp;g0.tab=1">vector(1)</a>
labels:
  severity: none
annotations:
  message: |
    This is an alert meant to ensure that the entire alerting pipeline is functional.
    This alert is always firing, therefore it should always be firing in Alertmanager
    and always fire against a receiver. There are integrations with various notification
    mechanisms that send a notification when this alert is not firing. For example the
    "DeadMansSnitch" integration in PagerDuty.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.795s ago
            </td>
            <td>270.4us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#k8s.rules" id="k8s.rules">k8s.rules</a></h2></td>
            <td><h2>26.893s ago</h2></td>
            <td><h2>547.2ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace%3Acontainer_cpu_usage_seconds_total%3Asum_rate&amp;g0.tab=1">namespace:container_cpu_usage_seconds_total:sum_rate</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%29+%28rate%28container_cpu_usage_seconds_total%7Bcontainer%21%3D%22POD%22%2Cimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D%5B5m%5D%29%29&amp;g0.tab=1">sum
  by(namespace) (rate(container_cpu_usage_seconds_total{container!="POD",image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[5m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.893s ago
            </td>
            <td>28.68ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_rate&amp;g0.tab=1">node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28cluster%2C+namespace%2C+pod%2C+container%29+%28rate%28container_cpu_usage_seconds_total%7Bcontainer%21%3D%22POD%22%2Cimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D%5B5m%5D%29%29+%2A+on%28cluster%2C+namespace%2C+pod%29+group_left%28node%29+topk+by%28cluster%2C+namespace%2C+pod%29+%281%2C+max+by%28cluster%2C+namespace%2C+pod%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">sum
  by(cluster, namespace, pod, container) (rate(container_cpu_usage_seconds_total{container!="POD",image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[5m]))
  * on(cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod)
  (1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.865s ago
            </td>
            <td>55.72ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod_container%3Acontainer_memory_working_set_bytes&amp;g0.tab=1">node_namespace_pod_container:container_memory_working_set_bytes</a>
expr: <a href="http://localhost:9090/graph?g0.expr=container_memory_working_set_bytes%7Bimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D+%2A+on%28namespace%2C+pod%29+group_left%28node%29+topk+by%28namespace%2C+pod%29+%281%2C+max+by%28namespace%2C+pod%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">container_memory_working_set_bytes{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}
  * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace,
  pod, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.809s ago
            </td>
            <td>87.49ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod_container%3Acontainer_memory_rss&amp;g0.tab=1">node_namespace_pod_container:container_memory_rss</a>
expr: <a href="http://localhost:9090/graph?g0.expr=container_memory_rss%7Bimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D+%2A+on%28namespace%2C+pod%29+group_left%28node%29+topk+by%28namespace%2C+pod%29+%281%2C+max+by%28namespace%2C+pod%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">container_memory_rss{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}
  * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace,
  pod, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.722s ago
            </td>
            <td>99.35ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod_container%3Acontainer_memory_cache&amp;g0.tab=1">node_namespace_pod_container:container_memory_cache</a>
expr: <a href="http://localhost:9090/graph?g0.expr=container_memory_cache%7Bimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D+%2A+on%28namespace%2C+pod%29+group_left%28node%29+topk+by%28namespace%2C+pod%29+%281%2C+max+by%28namespace%2C+pod%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">container_memory_cache{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}
  * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace,
  pod, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.622s ago
            </td>
            <td>88.77ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod_container%3Acontainer_memory_swap&amp;g0.tab=1">node_namespace_pod_container:container_memory_swap</a>
expr: <a href="http://localhost:9090/graph?g0.expr=container_memory_swap%7Bimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D+%2A+on%28namespace%2C+pod%29+group_left%28node%29+topk+by%28namespace%2C+pod%29+%281%2C+max+by%28namespace%2C+pod%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">container_memory_swap{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}
  * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace,
  pod, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.534s ago
            </td>
            <td>94.03ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace%3Acontainer_memory_usage_bytes%3Asum&amp;g0.tab=1">namespace:container_memory_usage_bytes:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%29+%28container_memory_usage_bytes%7Bcontainer%21%3D%22POD%22%2Cimage%21%3D%22%22%2Cjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%2Fcadvisor%22%7D%29&amp;g0.tab=1">sum
  by(namespace) (container_memory_usage_bytes{container!="POD",image!="",job="kubelet",metrics_path="/metrics/cadvisor"})</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.44s ago
            </td>
            <td>20.11ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace%3Akube_pod_container_resource_requests_memory_bytes%3Asum&amp;g0.tab=1">namespace:kube_pod_container_resource_requests_memory_bytes:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%29+%28sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%2C+container%29+%28kube_pod_container_resource_requests_memory_bytes%7Bjob%3D%22kube-state-metrics%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28%29+max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bphase%3D~%22Pending%7CRunning%22%7D+%3D%3D+1%29%29%29&amp;g0.tab=1">sum
  by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"})
  * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~"Pending|Running"}
  == 1)))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.42s ago
            </td>
            <td>25.83ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace%3Akube_pod_container_resource_requests_cpu_cores%3Asum&amp;g0.tab=1">namespace:kube_pod_container_resource_requests_cpu_cores:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%29+%28sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%2C+container%29+%28kube_pod_container_resource_requests_cpu_cores%7Bjob%3D%22kube-state-metrics%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28%29+max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bphase%3D~%22Pending%7CRunning%22%7D+%3D%3D+1%29%29%29&amp;g0.tab=1">sum
  by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"})
  * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~"Pending|Running"}
  == 1)))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.394s ago
            </td>
            <td>24.76ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace_workload_pod%3Akube_pod_owner%3Arelabel&amp;g0.tab=1">namespace_workload_pod:kube_pod_owner:relabel</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max+by%28cluster%2C+namespace%2C+workload%2C+pod%29+%28label_replace%28label_replace%28kube_pod_owner%7Bjob%3D%22kube-state-metrics%22%2Cowner_kind%3D%22ReplicaSet%22%7D%2C+%22replicaset%22%2C+%22%241%22%2C+%22owner_name%22%2C+%22%28.%2A%29%22%29+%2A+on%28replicaset%2C+namespace%29+group_left%28owner_name%29+topk+by%28replicaset%2C+namespace%29+%281%2C+max+by%28replicaset%2C+namespace%2C+owner_name%29+%28kube_replicaset_owner%7Bjob%3D%22kube-state-metrics%22%7D%29%29%2C+%22workload%22%2C+%22%241%22%2C+%22owner_name%22%2C+%22%28.%2A%29%22%29%29&amp;g0.tab=1">max
  by(cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics",owner_kind="ReplicaSet"},
  "replicaset", "$1", "owner_name", "(.*)") * on(replicaset,
  namespace) group_left(owner_name) topk by(replicaset, namespace) (1, max by(replicaset,
  namespace, owner_name) (kube_replicaset_owner{job="kube-state-metrics"})),
  "workload", "$1", "owner_name", "(.*)"))</a>
labels:
  workload_type: deployment
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.369s ago
            </td>
            <td>9.53ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace_workload_pod%3Akube_pod_owner%3Arelabel&amp;g0.tab=1">namespace_workload_pod:kube_pod_owner:relabel</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max+by%28cluster%2C+namespace%2C+workload%2C+pod%29+%28label_replace%28kube_pod_owner%7Bjob%3D%22kube-state-metrics%22%2Cowner_kind%3D%22DaemonSet%22%7D%2C+%22workload%22%2C+%22%241%22%2C+%22owner_name%22%2C+%22%28.%2A%29%22%29%29&amp;g0.tab=1">max
  by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job="kube-state-metrics",owner_kind="DaemonSet"},
  "workload", "$1", "owner_name", "(.*)"))</a>
labels:
  workload_type: daemonset
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.36s ago
            </td>
            <td>9.176ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=namespace_workload_pod%3Akube_pod_owner%3Arelabel&amp;g0.tab=1">namespace_workload_pod:kube_pod_owner:relabel</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max+by%28cluster%2C+namespace%2C+workload%2C+pod%29+%28label_replace%28kube_pod_owner%7Bjob%3D%22kube-state-metrics%22%2Cowner_kind%3D%22StatefulSet%22%7D%2C+%22workload%22%2C+%22%241%22%2C+%22owner_name%22%2C+%22%28.%2A%29%22%29%29&amp;g0.tab=1">max
  by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job="kube-state-metrics",owner_kind="StatefulSet"},
  "workload", "$1", "owner_name", "(.*)"))</a>
labels:
  workload_type: statefulset
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              26.351s ago
            </td>
            <td>3.673ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-apiserver-availability.rules" id="kube-apiserver-availability.rules">kube-apiserver-availability.rules</a></h2></td>
            <td><h2>2m18.022s ago</h2></td>
            <td><h2>10.73s</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aavailability30d&amp;g0.tab=1">apiserver_request:availability30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=1+-+%28%28sum%28increase%28apiserver_request_duration_seconds_count%7Bverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30d%5D%29%29+-+sum%28increase%28apiserver_request_duration_seconds_bucket%7Ble%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30d%5D%29%29%29+%2B+%28sum%28increase%28apiserver_request_duration_seconds_count%7Bverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+-+%28%28sum%28increase%28apiserver_request_duration_seconds_bucket%7Ble%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+or+vector%280%29%29+%2B+sum%28increase%28apiserver_request_duration_seconds_bucket%7Ble%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+%2B+sum%28increase%28apiserver_request_duration_seconds_bucket%7Ble%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29%29%29+%2B+sum%28code%3Aapiserver_request_total%3Aincrease30d%7Bcode%3D~%225..%22%7D+or+vector%280%29%29%29+%2F+sum%28code%3Aapiserver_request_total%3Aincrease30d%29&amp;g0.tab=1">1
  - ((sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
  - sum(increase(apiserver_request_duration_seconds_bucket{le="1",verb=~"POST|PUT|PATCH|DELETE"}[30d])))
  + (sum(increase(apiserver_request_duration_seconds_count{verb=~"LIST|GET"}[30d]))
  - ((sum(increase(apiserver_request_duration_seconds_bucket{le="0.1",scope=~"resource|",verb=~"LIST|GET"}[30d]))
  or vector(0)) + sum(increase(apiserver_request_duration_seconds_bucket{le="0.5",scope="namespace",verb=~"LIST|GET"}[30d]))
  + sum(increase(apiserver_request_duration_seconds_bucket{le="5",scope="cluster",verb=~"LIST|GET"}[30d]))))
  + sum(code:apiserver_request_total:increase30d{code=~"5.."} or vector(0)))
  / sum(code:apiserver_request_total:increase30d)</a>
labels:
  verb: all
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m18.022s ago
            </td>
            <td>3.758s</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aavailability30d&amp;g0.tab=1">apiserver_request:availability30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=1+-+%28sum%28increase%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+-+%28%28sum%28increase%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+or+vector%280%29%29+%2B+sum%28increase%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29+%2B+sum%28increase%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30d%5D%29%29%29+%2B+sum%28code%3Aapiserver_request_total%3Aincrease30d%7Bcode%3D~%225..%22%2Cverb%3D%22read%22%7D+or+vector%280%29%29%29+%2F+sum%28code%3Aapiserver_request_total%3Aincrease30d%7Bverb%3D%22read%22%7D%29&amp;g0.tab=1">1
  - (sum(increase(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30d]))
  - ((sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[30d]))
  or vector(0)) + sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[30d]))
  + sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[30d])))
  + sum(code:apiserver_request_total:increase30d{code=~"5..",verb="read"}
  or vector(0))) / sum(code:apiserver_request_total:increase30d{verb="read"})</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m14.264s ago
            </td>
            <td>2.815s</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aavailability30d&amp;g0.tab=1">apiserver_request:availability30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=1+-+%28%28sum%28increase%28apiserver_request_duration_seconds_count%7Bverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30d%5D%29%29+-+sum%28increase%28apiserver_request_duration_seconds_bucket%7Ble%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30d%5D%29%29%29+%2B+sum%28code%3Aapiserver_request_total%3Aincrease30d%7Bcode%3D~%225..%22%2Cverb%3D%22write%22%7D+or+vector%280%29%29%29+%2F+sum%28code%3Aapiserver_request_total%3Aincrease30d%7Bverb%3D%22write%22%7D%29&amp;g0.tab=1">1
  - ((sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
  - sum(increase(apiserver_request_duration_seconds_bucket{le="1",verb=~"POST|PUT|PATCH|DELETE"}[30d])))
  + sum(code:apiserver_request_total:increase30d{code=~"5..",verb="write"}
  or vector(0))) / sum(code:apiserver_request_total:increase30d{verb="write"})</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m11.45s ago
            </td>
            <td>937.3ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22LIST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="LIST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m10.513s ago
            </td>
            <td>1.579s</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22GET%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="GET"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m8.934s ago
            </td>
            <td>509.5ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22POST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="POST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m8.424s ago
            </td>
            <td>117.3ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PUT%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="PUT"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m8.307s ago
            </td>
            <td>156.8ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PATCH%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="PATCH"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m8.15s ago
            </td>
            <td>151.1ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%222..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22DELETE%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"2..",job="apiserver",verb="DELETE"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.999s ago
            </td>
            <td>88.66ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22LIST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="LIST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.911s ago
            </td>
            <td>1.969ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22GET%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="GET"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.909s ago
            </td>
            <td>1.43ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22POST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="POST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.907s ago
            </td>
            <td>932.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PUT%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="PUT"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.907s ago
            </td>
            <td>742.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PATCH%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="PATCH"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.906s ago
            </td>
            <td>816.4us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%223..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22DELETE%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"3..",job="apiserver",verb="DELETE"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.905s ago
            </td>
            <td>720us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22LIST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="LIST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.904s ago
            </td>
            <td>128.4ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22GET%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="GET"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.776s ago
            </td>
            <td>92.18ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22POST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="POST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.684s ago
            </td>
            <td>33.87ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PUT%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="PUT"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.65s ago
            </td>
            <td>61.06ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PATCH%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="PATCH"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.589s ago
            </td>
            <td>26.44ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%224..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22DELETE%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"4..",job="apiserver",verb="DELETE"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.563s ago
            </td>
            <td>28.85ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22LIST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="LIST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.534s ago
            </td>
            <td>180.2ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22GET%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="GET"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.354s ago
            </td>
            <td>18.08ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22POST%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="POST"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.336s ago
            </td>
            <td>13.58ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PUT%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="PUT"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.323s ago
            </td>
            <td>13.78ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22PATCH%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="PATCH"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.309s ago
            </td>
            <td>8.325ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_verb%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code_verb:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+verb%29+%28increase%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D%22DELETE%22%7D%5B30d%5D%29%29&amp;g0.tab=1">sum
  by(code, verb) (increase(apiserver_request_total{code=~"5..",job="apiserver",verb="DELETE"}[30d]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.301s ago
            </td>
            <td>1.412ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%29+%28code_verb%3Aapiserver_request_total%3Aincrease30d%7Bverb%3D~%22LIST%7CGET%22%7D%29&amp;g0.tab=1">sum
  by(code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.299s ago
            </td>
            <td>197.4us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code%3Aapiserver_request_total%3Aincrease30d&amp;g0.tab=1">code:apiserver_request_total:increase30d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%29+%28code_verb%3Aapiserver_request_total%3Aincrease30d%7Bverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%29&amp;g0.tab=1">sum
  by(code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2m7.299s ago
            </td>
            <td>238us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-apiserver-slos" id="kube-apiserver-slos">kube-apiserver-slos</a></h2></td>
            <td><h2>6.199s ago</h2></td>
            <td><h2>1.016ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeAPIErrorBudgetBurn%22%7D&amp;g0.tab=1">KubeAPIErrorBudgetBurn</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28apiserver_request%3Aburnrate1h%29+%3E+%2814.4+%2A+0.01%29+and+sum%28apiserver_request%3Aburnrate5m%29+%3E+%2814.4+%2A+0.01%29&amp;g0.tab=1">sum(apiserver_request:burnrate1h)
  &gt; (14.4 * 0.01) and sum(apiserver_request:burnrate5m) &gt; (14.4 * 0.01)</a>
for: 2m
labels:
  long: 1h
  severity: critical
  short: 5m
annotations:
  message: The API server is burning too much error budget
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.199s ago
            </td>
            <td>464.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeAPIErrorBudgetBurn%22%7D&amp;g0.tab=1">KubeAPIErrorBudgetBurn</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28apiserver_request%3Aburnrate6h%29+%3E+%286+%2A+0.01%29+and+sum%28apiserver_request%3Aburnrate30m%29+%3E+%286+%2A+0.01%29&amp;g0.tab=1">sum(apiserver_request:burnrate6h)
  &gt; (6 * 0.01) and sum(apiserver_request:burnrate30m) &gt; (6 * 0.01)</a>
for: 15m
labels:
  long: 6h
  severity: critical
  short: 30m
annotations:
  message: The API server is burning too much error budget
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.199s ago
            </td>
            <td>208.7us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeAPIErrorBudgetBurn%22%7D&amp;g0.tab=1">KubeAPIErrorBudgetBurn</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28apiserver_request%3Aburnrate1d%29+%3E+%283+%2A+0.01%29+and+sum%28apiserver_request%3Aburnrate2h%29+%3E+%283+%2A+0.01%29&amp;g0.tab=1">sum(apiserver_request:burnrate1d)
  &gt; (3 * 0.01) and sum(apiserver_request:burnrate2h) &gt; (3 * 0.01)</a>
for: 1h
labels:
  long: 1d
  severity: warning
  short: 2h
annotations:
  message: The API server is burning too much error budget
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.199s ago
            </td>
            <td>170.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeAPIErrorBudgetBurn%22%7D&amp;g0.tab=1">KubeAPIErrorBudgetBurn</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28apiserver_request%3Aburnrate3d%29+%3E+%281+%2A+0.01%29+and+sum%28apiserver_request%3Aburnrate6h%29+%3E+%281+%2A+0.01%29&amp;g0.tab=1">sum(apiserver_request:burnrate3d)
  &gt; (1 * 0.01) and sum(apiserver_request:burnrate6h) &gt; (1 * 0.01)</a>
for: 3h
labels:
  long: 3d
  severity: warning
  short: 6h
annotations:
  message: The API server is burning too much error budget
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              6.199s ago
            </td>
            <td>157.5us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-apiserver.rules" id="kube-apiserver.rules">kube-apiserver.rules</a></h2></td>
            <td><h2>4.734s ago</h2></td>
            <td><h2>4.409s</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate1d&amp;g0.tab=1">apiserver_request:burnrate1d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1d%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1d]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[1d]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[1d]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[1d]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[1d])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.734s ago
            </td>
            <td>635.7ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate1h&amp;g0.tab=1">apiserver_request:burnrate1h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B1h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1h]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[1h]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[1h]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[1h]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[1h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.099s ago
            </td>
            <td>41.46ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate2h&amp;g0.tab=1">apiserver_request:burnrate2h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B2h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[2h]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[2h]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[2h]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[2h]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[2h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.057s ago
            </td>
            <td>63.14ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate30m&amp;g0.tab=1">apiserver_request:burnrate30m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B30m%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30m]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[30m]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[30m]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[30m]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[30m])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              3.994s ago
            </td>
            <td>38.16ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate3d&amp;g0.tab=1">apiserver_request:burnrate3d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B3d%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[3d]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[3d]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[3d]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[3d]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[3d])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              3.956s ago
            </td>
            <td>1.738s</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate5m&amp;g0.tab=1">apiserver_request:burnrate5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[5m]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[5m]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[5m]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[5m]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[5m])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2.219s ago
            </td>
            <td>21.18ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate6h&amp;g0.tab=1">apiserver_request:burnrate6h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29+-+%28%28sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.1%22%2Cscope%3D~%22resource%7C%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29+or+vector%280%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%220.5%22%2Cscope%3D%22namespace%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29+%2B+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%225%22%2Cscope%3D%22cluster%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B6h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[6h]))
  - ((sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.1",scope=~"resource|",verb=~"LIST|GET"}[6h]))
  or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="0.5",scope="namespace",verb=~"LIST|GET"}[6h]))
  + sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="5",scope="cluster",verb=~"LIST|GET"}[6h]))))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"LIST|GET"}[6h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2.198s ago
            </td>
            <td>180.6ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate1d&amp;g0.tab=1">apiserver_request:burnrate1d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1d%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1d%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1d%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1d%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[1d])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              2.017s ago
            </td>
            <td>195.1ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate1h&amp;g0.tab=1">apiserver_request:burnrate1h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1h%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1h%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B1h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[1h])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.822s ago
            </td>
            <td>13.25ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate2h&amp;g0.tab=1">apiserver_request:burnrate2h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B2h%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B2h%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B2h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B2h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[2h])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.809s ago
            </td>
            <td>20ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate30m&amp;g0.tab=1">apiserver_request:burnrate30m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30m%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30m%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30m%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B30m%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[30m])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.789s ago
            </td>
            <td>12.72ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate3d&amp;g0.tab=1">apiserver_request:burnrate3d</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B3d%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B3d%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B3d%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B3d%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[3d])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.777s ago
            </td>
            <td>525ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate5m&amp;g0.tab=1">apiserver_request:burnrate5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[5m])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.252s ago
            </td>
            <td>9.669ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=apiserver_request%3Aburnrate6h&amp;g0.tab=1">apiserver_request:burnrate6h</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28%28sum%28rate%28apiserver_request_duration_seconds_count%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B6h%5D%29%29+-+sum%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cle%3D%221%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B6h%5D%29%29%29+%2B+sum%28rate%28apiserver_request_total%7Bcode%3D~%225..%22%2Cjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B6h%5D%29%29%29+%2F+sum%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B6h%5D%29%29&amp;g0.tab=1">((sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",le="1",verb=~"POST|PUT|PATCH|DELETE"}[6h])))
  + sum(rate(apiserver_request_total{code=~"5..",job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h])))
  / sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.242s ago
            </td>
            <td>59.4ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_resource%3Aapiserver_request_total%3Arate5m&amp;g0.tab=1">code_resource:apiserver_request_total:rate5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+resource%29+%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29&amp;g0.tab=1">sum
  by(code, resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))</a>
labels:
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.183s ago
            </td>
            <td>10.62ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=code_resource%3Aapiserver_request_total%3Arate5m&amp;g0.tab=1">code_resource:apiserver_request_total:rate5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28code%2C+resource%29+%28rate%28apiserver_request_total%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29&amp;g0.tab=1">sum
  by(code, resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))</a>
labels:
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.172s ago
            </td>
            <td>4.019ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:apiserver_request_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+by%28le%2C+resource%29+%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cverb%3D~%22LIST%7CGET%22%7D%5B5m%5D%29%29%29+%3E+0&amp;g0.tab=1">histogram_quantile(0.99,
  sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m])))
  &gt; 0</a>
labels:
  quantile: "0.99"
  verb: read
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              1.169s ago
            </td>
            <td>178.4ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:apiserver_request_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+by%28le%2C+resource%29+%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Cverb%3D~%22POST%7CPUT%7CPATCH%7CDELETE%22%7D%5B5m%5D%29%29%29+%3E+0&amp;g0.tab=1">histogram_quantile(0.99,
  sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m])))
  &gt; 0</a>
labels:
  quantile: "0.99"
  verb: write
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              990ms ago
            </td>
            <td>61.66ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster%3Aapiserver_request_duration_seconds%3Amean5m&amp;g0.tab=1">cluster:apiserver_request_duration_seconds:mean5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28instance%2C+pod%29+%28rate%28apiserver_request_duration_seconds_sum%7Bsubresource%21%3D%22log%22%2Cverb%21~%22LIST%7CWATCH%7CWATCHLIST%7CDELETECOLLECTION%7CPROXY%7CCONNECT%22%7D%5B5m%5D%29%29+%2F+sum+without%28instance%2C+pod%29+%28rate%28apiserver_request_duration_seconds_count%7Bsubresource%21%3D%22log%22%2Cverb%21~%22LIST%7CWATCH%7CWATCHLIST%7CDELETECOLLECTION%7CPROXY%7CCONNECT%22%7D%5B5m%5D%29%29&amp;g0.tab=1">sum
  without(instance, pod) (rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m]))
  / sum without(instance, pod) (rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              929ms ago
            </td>
            <td>18.4ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:apiserver_request_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+without%28instance%2C+pod%29+%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Csubresource%21%3D%22log%22%2Cverb%21~%22LIST%7CWATCH%7CWATCHLIST%7CDELETECOLLECTION%7CPROXY%7CCONNECT%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.99,
  sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])))</a>
labels:
  quantile: "0.99"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              910ms ago
            </td>
            <td>193.7ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:apiserver_request_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.9%2C+sum+without%28instance%2C+pod%29+%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Csubresource%21%3D%22log%22%2Cverb%21~%22LIST%7CWATCH%7CWATCHLIST%7CDELETECOLLECTION%7CPROXY%7CCONNECT%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.9,
  sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])))</a>
labels:
  quantile: "0.9"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              717ms ago
            </td>
            <td>197.5ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:apiserver_request_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.5%2C+sum+without%28instance%2C+pod%29+%28rate%28apiserver_request_duration_seconds_bucket%7Bjob%3D%22apiserver%22%2Csubresource%21%3D%22log%22%2Cverb%21~%22LIST%7CWATCH%7CWATCHLIST%7CDELETECOLLECTION%7CPROXY%7CCONNECT%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.5,
  sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])))</a>
labels:
  quantile: "0.5"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              519ms ago
            </td>
            <td>191.6ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-prometheus-general.rules" id="kube-prometheus-general.rules">kube-prometheus-general.rules</a></h2></td>
            <td><h2>13.278s ago</h2></td>
            <td><h2>3.231ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=count%3Aup1&amp;g0.tab=1">count:up1</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+without%28instance%2C+pod%2C+node%29+%28up+%3D%3D+1%29&amp;g0.tab=1">count
  without(instance, pod, node) (up == 1)</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              13.278s ago
            </td>
            <td>2.064ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=count%3Aup0&amp;g0.tab=1">count:up0</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+without%28instance%2C+pod%2C+node%29+%28up+%3D%3D+0%29&amp;g0.tab=1">count
  without(instance, pod, node) (up == 0)</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              13.276s ago
            </td>
            <td>1.158ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-prometheus-node-recording.rules" id="kube-prometheus-node-recording.rules">kube-prometheus-node-recording.rules</a></h2></td>
            <td><h2>23.254s ago</h2></td>
            <td><h2>195.4ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_cpu%3Arate%3Asum&amp;g0.tab=1">instance:node_cpu:rate:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28instance%29+%28rate%28node_cpu_seconds_total%7Bmode%21%3D%22idle%22%2Cmode%21%3D%22iowait%22%7D%5B3m%5D%29%29&amp;g0.tab=1">sum
  by(instance) (rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.254s ago
            </td>
            <td>33.02ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_receive_bytes%3Arate%3Asum&amp;g0.tab=1">instance:node_network_receive_bytes:rate:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28instance%29+%28rate%28node_network_receive_bytes_total%5B3m%5D%29%29&amp;g0.tab=1">sum
  by(instance) (rate(node_network_receive_bytes_total[3m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.221s ago
            </td>
            <td>8.134ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_transmit_bytes%3Arate%3Asum&amp;g0.tab=1">instance:node_network_transmit_bytes:rate:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28instance%29+%28rate%28node_network_transmit_bytes_total%5B3m%5D%29%29&amp;g0.tab=1">sum
  by(instance) (rate(node_network_transmit_bytes_total[3m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.213s ago
            </td>
            <td>8.279ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_cpu%3Aratio&amp;g0.tab=1">instance:node_cpu:ratio</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28cpu%2C+mode%29+%28rate%28node_cpu_seconds_total%7Bmode%21%3D%22idle%22%2Cmode%21%3D%22iowait%22%7D%5B5m%5D%29%29+%2F+on%28instance%29+group_left%28%29+count+by%28instance%29+%28sum+by%28instance%2C+cpu%29+%28node_cpu_seconds_total%29%29&amp;g0.tab=1">sum
  without(cpu, mode) (rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
  / on(instance) group_left() count by(instance) (sum by(instance, cpu) (node_cpu_seconds_total))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.204s ago
            </td>
            <td>73.4ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster%3Anode_cpu%3Asum_rate5m&amp;g0.tab=1">cluster:node_cpu:sum_rate5m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28rate%28node_cpu_seconds_total%7Bmode%21%3D%22idle%22%2Cmode%21%3D%22iowait%22%7D%5B5m%5D%29%29&amp;g0.tab=1">sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.131s ago
            </td>
            <td>36.11ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster%3Anode_cpu%3Aratio&amp;g0.tab=1">cluster:node_cpu:ratio</a>
expr: <a href="http://localhost:9090/graph?g0.expr=cluster%3Anode_cpu_seconds_total%3Arate5m+%2F+count%28sum+by%28instance%2C+cpu%29+%28node_cpu_seconds_total%29%29&amp;g0.tab=1">cluster:node_cpu_seconds_total:rate5m
  / count(sum by(instance, cpu) (node_cpu_seconds_total))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.095s ago
            </td>
            <td>36.46ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-scheduler.rules" id="kube-scheduler.rules">kube-scheduler.rules</a></h2></td>
            <td><h2>8.407s ago</h2></td>
            <td><h2>993us</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_e2e_scheduling_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_e2e_scheduling_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.99,
  sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.99"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.407s ago
            </td>
            <td>317.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_scheduling_algorithm_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_scheduling_algorithm_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.99,
  sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.99"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>94.11us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_binding_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_binding_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.99,
  sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.99"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>88.31us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_e2e_scheduling_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.9%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_e2e_scheduling_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.9,
  sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.9"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>75.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_scheduling_algorithm_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.9%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_scheduling_algorithm_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.9,
  sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.9"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>74.81us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_binding_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.9%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_binding_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.9,
  sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.9"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>77.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_e2e_scheduling_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.5%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_e2e_scheduling_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.5,
  sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.5"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>69.41us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_scheduling_algorithm_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.5%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_scheduling_algorithm_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.5,
  sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.5"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.406s ago
            </td>
            <td>65.4us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=cluster_quantile%3Ascheduler_binding_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.5%2C+sum+without%28instance%2C+pod%29+%28rate%28scheduler_binding_duration_seconds_bucket%7Bjob%3D%22kube-scheduler%22%7D%5B5m%5D%29%29%29&amp;g0.tab=1">histogram_quantile(0.5,
  sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])))</a>
labels:
  quantile: "0.5"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.407s ago
            </td>
            <td>113us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kube-state-metrics" id="kube-state-metrics">kube-state-metrics</a></h2></td>
            <td><h2>19.693s ago</h2></td>
            <td><h2>524.3us</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeStateMetricsListErrors%22%7D&amp;g0.tab=1">KubeStateMetricsListErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28sum%28rate%28kube_state_metrics_list_total%7Bjob%3D%22kube-state-metrics%22%2Cresult%3D%22error%22%7D%5B5m%5D%29%29+%2F+sum%28rate%28kube_state_metrics_list_total%7Bjob%3D%22kube-state-metrics%22%7D%5B5m%5D%29%29%29+%3E+0.01&amp;g0.tab=1">(sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
  / sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
  &gt; 0.01</a>
for: 15m
labels:
  severity: critical
annotations:
  message: kube-state-metrics is experiencing errors at an elevated rate in list operations.
    This is likely causing it to not be able to expose metrics about Kubernetes objects
    correctly or at all.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricslisterrors
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              19.693s ago
            </td>
            <td>389.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeStateMetricsWatchErrors%22%7D&amp;g0.tab=1">KubeStateMetricsWatchErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28sum%28rate%28kube_state_metrics_watch_total%7Bjob%3D%22kube-state-metrics%22%2Cresult%3D%22error%22%7D%5B5m%5D%29%29+%2F+sum%28rate%28kube_state_metrics_watch_total%7Bjob%3D%22kube-state-metrics%22%7D%5B5m%5D%29%29%29+%3E+0.01&amp;g0.tab=1">(sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
  / sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
  &gt; 0.01</a>
for: 15m
labels:
  severity: critical
annotations:
  message: kube-state-metrics is experiencing errors at an elevated rate in watch
    operations. This is likely causing it to not be able to expose metrics about Kubernetes
    objects correctly or at all.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricswatcherrors
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              19.693s ago
            </td>
            <td>125.9us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubelet.rules" id="kubelet.rules">kubelet.rules</a></h2></td>
            <td><h2>18.276s ago</h2></td>
            <td><h2>19.08ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_quantile%3Akubelet_pleg_relist_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+by%28instance%2C+le%29+%28rate%28kubelet_pleg_relist_duration_seconds_bucket%5B5m%5D%29%29+%2A+on%28instance%29+group_left%28node%29+kubelet_node_name%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D%29&amp;g0.tab=1">histogram_quantile(0.99,
  sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance)
  group_left(node) kubelet_node_name{job="kubelet",metrics_path="/metrics"})</a>
labels:
  quantile: "0.99"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.276s ago
            </td>
            <td>6.783ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_quantile%3Akubelet_pleg_relist_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.9%2C+sum+by%28instance%2C+le%29+%28rate%28kubelet_pleg_relist_duration_seconds_bucket%5B5m%5D%29%29+%2A+on%28instance%29+group_left%28node%29+kubelet_node_name%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D%29&amp;g0.tab=1">histogram_quantile(0.9,
  sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance)
  group_left(node) kubelet_node_name{job="kubelet",metrics_path="/metrics"})</a>
labels:
  quantile: "0.9"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.27s ago
            </td>
            <td>6.515ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_quantile%3Akubelet_pleg_relist_duration_seconds%3Ahistogram_quantile&amp;g0.tab=1">node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.5%2C+sum+by%28instance%2C+le%29+%28rate%28kubelet_pleg_relist_duration_seconds_bucket%5B5m%5D%29%29+%2A+on%28instance%29+group_left%28node%29+kubelet_node_name%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D%29&amp;g0.tab=1">histogram_quantile(0.5,
  sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance)
  group_left(node) kubelet_node_name{job="kubelet",metrics_path="/metrics"})</a>
labels:
  quantile: "0.5"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.263s ago
            </td>
            <td>5.766ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-apps" id="kubernetes-apps">kubernetes-apps</a></h2></td>
            <td><h2>4.534s ago</h2></td>
            <td><h2>140.2ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubePodCrashLooping%22%7D&amp;g0.tab=1">KubePodCrashLooping</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28kube_pod_container_status_restarts_total%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%5B5m%5D%29+%2A+60+%2A+5+%3E+0&amp;g0.tab=1">rate(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace=~".*"}[5m])
  * 60 * 5 &gt; 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }})
    is restarting {{ printf "%.2f" $value }} times / 5 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.534s ago
            </td>
            <td>16.59ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubePodNotReady%22%7D&amp;g0.tab=1">KubePodNotReady</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%2Cphase%3D~%22Pending%7CUnknown%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28owner_kind%29+topk+by%28namespace%2C+pod%29+%281%2C+max+by%28namespace%2C+pod%2C+owner_kind%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29%29+%3E+0&amp;g0.tab=1">sum
  by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics",namespace=~".*",phase=~"Pending|Unknown"})
  * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace,
  pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) &gt; 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state
    for longer than 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.518s ago
            </td>
            <td>30.58ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeDeploymentGenerationMismatch%22%7D&amp;g0.tab=1">KubeDeploymentGenerationMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_deployment_status_observed_generation%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_deployment_metadata_generation%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D&amp;g0.tab=1">kube_deployment_status_observed_generation{job="kube-state-metrics",namespace=~".*"}
  != kube_deployment_metadata_generation{job="kube-state-metrics",namespace=~".*"}</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
    }} does not match, this indicates that the Deployment has failed but has not been
    rolled back.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.487s ago
            </td>
            <td>3.424ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeDeploymentReplicasMismatch%22%7D&amp;g0.tab=1">KubeDeploymentReplicasMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28kube_deployment_spec_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_deployment_status_replicas_available%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+and+%28changes%28kube_deployment_status_replicas_updated%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%5B5m%5D%29+%3D%3D+0%29&amp;g0.tab=1">(kube_deployment_spec_replicas{job="kube-state-metrics",namespace=~".*"}
  != kube_deployment_status_replicas_available{job="kube-state-metrics",namespace=~".*"})
  and (changes(kube_deployment_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}[5m])
  == 0)</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched
    the expected number of replicas for longer than 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.484s ago
            </td>
            <td>4.098ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeStatefulSetReplicasMismatch%22%7D&amp;g0.tab=1">KubeStatefulSetReplicasMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28kube_statefulset_status_replicas_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_statefulset_status_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+and+%28changes%28kube_statefulset_status_replicas_updated%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%5B5m%5D%29+%3D%3D+0%29&amp;g0.tab=1">(kube_statefulset_status_replicas_ready{job="kube-state-metrics",namespace=~".*"}
  != kube_statefulset_status_replicas{job="kube-state-metrics",namespace=~".*"})
  and (changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}[5m])
  == 0)</a>
for: 15m
labels:
  severity: warning
annotations:
  message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched
    the expected number of replicas for longer than 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.48s ago
            </td>
            <td>6.871ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeStatefulSetGenerationMismatch%22%7D&amp;g0.tab=1">KubeStatefulSetGenerationMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_statefulset_status_observed_generation%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_statefulset_metadata_generation%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D&amp;g0.tab=1">kube_statefulset_status_observed_generation{job="kube-state-metrics",namespace=~".*"}
  != kube_statefulset_metadata_generation{job="kube-state-metrics",namespace=~".*"}</a>
for: 15m
labels:
  severity: warning
annotations:
  message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
    }} does not match, this indicates that the StatefulSet has failed but has not
    been rolled back.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.473s ago
            </td>
            <td>2.531ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeStatefulSetUpdateNotRolledOut%22%7D&amp;g0.tab=1">KubeStatefulSetUpdateNotRolledOut</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28max+without%28revision%29+%28kube_statefulset_status_current_revision%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+unless+kube_statefulset_status_update_revision%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+%2A+%28kube_statefulset_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_statefulset_status_replicas_updated%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29%29+and+%28changes%28kube_statefulset_status_replicas_updated%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%5B5m%5D%29+%3D%3D+0%29&amp;g0.tab=1">(max
  without(revision) (kube_statefulset_status_current_revision{job="kube-state-metrics",namespace=~".*"}
  unless kube_statefulset_status_update_revision{job="kube-state-metrics",namespace=~".*"})
  * (kube_statefulset_replicas{job="kube-state-metrics",namespace=~".*"}
  != kube_statefulset_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}))
  and (changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}[5m])
  == 0)</a>
for: 15m
labels:
  severity: warning
annotations:
  message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has
    not been rolled out.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.471s ago
            </td>
            <td>6.296ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeDaemonSetRolloutStuck%22%7D&amp;g0.tab=1">KubeDaemonSetRolloutStuck</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%2F+kube_daemonset_status_desired_number_scheduled%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3C+1&amp;g0.tab=1">kube_daemonset_status_number_ready{job="kube-state-metrics",namespace=~".*"}
  / kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics",namespace=~".*"}
  &lt; 1</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Only {{ $value | humanizePercentage }} of the desired Pods of DaemonSet
    {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.465s ago
            </td>
            <td>640.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeContainerWaiting%22%7D&amp;g0.tab=1">KubeContainerWaiting</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28namespace%2C+pod%2C+container%29+%28kube_pod_container_status_waiting_reason%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+%3E+0&amp;g0.tab=1">sum
  by(namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",namespace=~".*"})
  &gt; 0</a>
for: 1h
labels:
  severity: warning
annotations:
  message: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}}
    has been in waiting state for longer than 1 hour.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.464s ago
            </td>
            <td>61.75ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeDaemonSetNotScheduled%22%7D&amp;g0.tab=1">KubeDaemonSetNotScheduled</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_daemonset_status_desired_number_scheduled%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+-+kube_daemonset_status_current_number_scheduled%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3E+0&amp;g0.tab=1">kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics",namespace=~".*"}
  - kube_daemonset_status_current_number_scheduled{job="kube-state-metrics",namespace=~".*"}
  &gt; 0</a>
for: 10m
labels:
  severity: warning
annotations:
  message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
    }} are not scheduled.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.403s ago
            </td>
            <td>1.091ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeDaemonSetMisScheduled%22%7D&amp;g0.tab=1">KubeDaemonSetMisScheduled</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_daemonset_status_number_misscheduled%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3E+0&amp;g0.tab=1">kube_daemonset_status_number_misscheduled{job="kube-state-metrics",namespace=~".*"}
  &gt; 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
    }} are running where they are not supposed to run.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.402s ago
            </td>
            <td>355.3us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeJobCompletion%22%7D&amp;g0.tab=1">KubeJobCompletion</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_job_spec_completions%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+-+kube_job_status_succeeded%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3E+0&amp;g0.tab=1">kube_job_spec_completions{job="kube-state-metrics",namespace=~".*"}
  - kube_job_status_succeeded{job="kube-state-metrics",namespace=~".*"}
  &gt; 0</a>
for: 12h
labels:
  severity: warning
annotations:
  message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than
    12 hours to complete.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.402s ago
            </td>
            <td>2.534ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeJobFailed%22%7D&amp;g0.tab=1">KubeJobFailed</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_job_failed%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3E+0&amp;g0.tab=1">kube_job_failed{job="kube-state-metrics",namespace=~".*"}
  &gt; 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.399s ago
            </td>
            <td>1.787ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeHpaReplicasMismatch%22%7D&amp;g0.tab=1">KubeHpaReplicasMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28kube_hpa_status_desired_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%21%3D+kube_hpa_status_current_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+and+changes%28kube_hpa_status_current_replicas%5B15m%5D%29+%3D%3D+0&amp;g0.tab=1">(kube_hpa_status_desired_replicas{job="kube-state-metrics",namespace=~".*"}
  != kube_hpa_status_current_replicas{job="kube-state-metrics",namespace=~".*"})
  and changes(kube_hpa_status_current_replicas[15m]) == 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired
    number of replicas for longer than 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.398s ago
            </td>
            <td>701.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeHpaMaxedOut%22%7D&amp;g0.tab=1">KubeHpaMaxedOut</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_hpa_status_current_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3D%3D+kube_hpa_spec_max_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22.%2A%22%7D&amp;g0.tab=1">kube_hpa_status_current_replicas{job="kube-state-metrics",namespace=~".*"}
  == kube_hpa_spec_max_replicas{job="kube-state-metrics",namespace=~".*"}</a>
for: 15m
labels:
  severity: warning
annotations:
  message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas
    for longer than 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.397s ago
            </td>
            <td>896.7us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-resources" id="kubernetes-resources">kubernetes-resources</a></h2></td>
            <td><h2>17.721s ago</h2></td>
            <td><h2>33.61ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeCPUOvercommit%22%7D&amp;g0.tab=1">KubeCPUOvercommit</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28namespace%3Akube_pod_container_resource_requests_cpu_cores%3Asum%29+%2F+sum%28kube_node_status_allocatable_cpu_cores%29+%3E+%28count%28kube_node_status_allocatable_cpu_cores%29+-+1%29+%2F+count%28kube_node_status_allocatable_cpu_cores%29&amp;g0.tab=1">sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
  / sum(kube_node_status_allocatable_cpu_cores) &gt; (count(kube_node_status_allocatable_cpu_cores)
  - 1) / count(kube_node_status_allocatable_cpu_cores)</a>
for: 5m
labels:
  severity: warning
annotations:
  message: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate
    node failure.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.721s ago
            </td>
            <td>1.507ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeMemoryOvercommit%22%7D&amp;g0.tab=1">KubeMemoryOvercommit</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28namespace%3Akube_pod_container_resource_requests_memory_bytes%3Asum%29+%2F+sum%28kube_node_status_allocatable_memory_bytes%29+%3E+%28count%28kube_node_status_allocatable_memory_bytes%29+-+1%29+%2F+count%28kube_node_status_allocatable_memory_bytes%29&amp;g0.tab=1">sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
  / sum(kube_node_status_allocatable_memory_bytes) &gt; (count(kube_node_status_allocatable_memory_bytes)
  - 1) / count(kube_node_status_allocatable_memory_bytes)</a>
for: 5m
labels:
  severity: warning
annotations:
  message: Cluster has overcommitted memory resource requests for Pods and cannot
    tolerate node failure.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryovercommit
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.72s ago
            </td>
            <td>1.125ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeCPUQuotaOvercommit%22%7D&amp;g0.tab=1">KubeCPUQuotaOvercommit</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28kube_resourcequota%7Bjob%3D%22kube-state-metrics%22%2Cresource%3D%22cpu%22%2Ctype%3D%22hard%22%7D%29+%2F+sum%28kube_node_status_allocatable_cpu_cores%29+%3E+1.5&amp;g0.tab=1">sum(kube_resourcequota{job="kube-state-metrics",resource="cpu",type="hard"})
  / sum(kube_node_status_allocatable_cpu_cores) &gt; 1.5</a>
for: 5m
labels:
  severity: warning
annotations:
  message: Cluster has overcommitted CPU resource requests for Namespaces.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuquotaovercommit
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.719s ago
            </td>
            <td>295.6us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeMemoryQuotaOvercommit%22%7D&amp;g0.tab=1">KubeMemoryQuotaOvercommit</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28kube_resourcequota%7Bjob%3D%22kube-state-metrics%22%2Cresource%3D%22memory%22%2Ctype%3D%22hard%22%7D%29+%2F+sum%28kube_node_status_allocatable_memory_bytes%7Bjob%3D%22node-exporter%22%7D%29+%3E+1.5&amp;g0.tab=1">sum(kube_resourcequota{job="kube-state-metrics",resource="memory",type="hard"})
  / sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"}) &gt;
  1.5</a>
for: 5m
labels:
  severity: warning
annotations:
  message: Cluster has overcommitted memory resource requests for Namespaces.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryquotaovercommit
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.719s ago
            </td>
            <td>97.31us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeQuotaFullyUsed%22%7D&amp;g0.tab=1">KubeQuotaFullyUsed</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_resourcequota%7Bjob%3D%22kube-state-metrics%22%2Ctype%3D%22used%22%7D+%2F+ignoring%28instance%2C+job%2C+type%29+%28kube_resourcequota%7Bjob%3D%22kube-state-metrics%22%2Ctype%3D%22hard%22%7D+%3E+0%29+%3E%3D+1&amp;g0.tab=1">kube_resourcequota{job="kube-state-metrics",type="used"}
  / ignoring(instance, job, type) (kube_resourcequota{job="kube-state-metrics",type="hard"}
  &gt; 0) &gt;= 1</a>
for: 15m
labels:
  severity: info
annotations:
  message: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
    }} of its {{ $labels.resource }} quota.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotafullyused
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.719s ago
            </td>
            <td>1.296ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22CPUThrottlingHigh%22%7D&amp;g0.tab=1">CPUThrottlingHigh</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29&amp;g0.tab=1">sum
  by(container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=""}[5m]))
  / sum by(container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m]))
  &gt; (25 / 100)</a>
for: 15m
labels:
  severity: info
annotations:
  message: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace
    }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              17.718s ago
            </td>
            <td>29.27ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-storage" id="kubernetes-storage">kubernetes-storage</a></h2></td>
            <td><h2>23.645s ago</h2></td>
            <td><h2>46.55ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubePersistentVolumeFillingUp%22%7D&amp;g0.tab=1">KubePersistentVolumeFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kubelet_volume_stats_available_bytes%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%2Cnamespace%3D~%22.%2A%22%7D+%2F+kubelet_volume_stats_capacity_bytes%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%2Cnamespace%3D~%22.%2A%22%7D+%3C+0.03&amp;g0.tab=1">kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"}
  / kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"}
  &lt; 0.03</a>
for: 1m
labels:
  severity: critical
annotations:
  message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in
    Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.645s ago
            </td>
            <td>5.577ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubePersistentVolumeFillingUp%22%7D&amp;g0.tab=1">KubePersistentVolumeFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28kubelet_volume_stats_available_bytes%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%2Cnamespace%3D~%22.%2A%22%7D+%2F+kubelet_volume_stats_capacity_bytes%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%2Cnamespace%3D~%22.%2A%22%7D%29+%3C+0.15+and+predict_linear%28kubelet_volume_stats_available_bytes%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%2Cnamespace%3D~%22.%2A%22%7D%5B6h%5D%2C+4+%2A+24+%2A+3600%29+%3C+0&amp;g0.tab=1">(kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"}
  / kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"})
  &lt; 0.15 and predict_linear(kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"}[6h],
  4 * 24 * 3600) &lt; 0</a>
for: 1h
labels:
  severity: warning
annotations:
  message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
    }} in Namespace {{ $labels.namespace }} is expected to fill up within four days.
    Currently {{ $value | humanizePercentage }} is available.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.64s ago
            </td>
            <td>32.99ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubePersistentVolumeErrors%22%7D&amp;g0.tab=1">KubePersistentVolumeErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_persistentvolume_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D~%22Failed%7CPending%22%7D+%3E+0&amp;g0.tab=1">kube_persistentvolume_status_phase{job="kube-state-metrics",phase=~"Failed|Pending"}
  &gt; 0</a>
for: 5m
labels:
  severity: critical
annotations:
  message: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase
    }}.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              23.607s ago
            </td>
            <td>7.963ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-system-apiserver" id="kubernetes-system-apiserver">kubernetes-system-apiserver</a></h2></td>
            <td><h2>27.128s ago</h2></td>
            <td><h2>1.772ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeClientCertificateExpiration%22%7D&amp;g0.tab=1">KubeClientCertificateExpiration</a>
expr: <a href="http://localhost:9090/graph?g0.expr=apiserver_client_certificate_expiration_seconds_count%7Bjob%3D%22apiserver%22%7D+%3E+0+and+on%28job%29+histogram_quantile%280.01%2C+sum+by%28job%2C+le%29+%28rate%28apiserver_client_certificate_expiration_seconds_bucket%7Bjob%3D%22apiserver%22%7D%5B5m%5D%29%29%29+%3C+604800&amp;g0.tab=1">apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
  &gt; 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
  &lt; 604800</a>
labels:
  severity: warning
annotations:
  message: A client certificate used to authenticate to the apiserver is expiring
    in less than 7.0 days.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              27.128s ago
            </td>
            <td>985.3us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeClientCertificateExpiration%22%7D&amp;g0.tab=1">KubeClientCertificateExpiration</a>
expr: <a href="http://localhost:9090/graph?g0.expr=apiserver_client_certificate_expiration_seconds_count%7Bjob%3D%22apiserver%22%7D+%3E+0+and+on%28job%29+histogram_quantile%280.01%2C+sum+by%28job%2C+le%29+%28rate%28apiserver_client_certificate_expiration_seconds_bucket%7Bjob%3D%22apiserver%22%7D%5B5m%5D%29%29%29+%3C+86400&amp;g0.tab=1">apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
  &gt; 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
  &lt; 86400</a>
labels:
  severity: critical
annotations:
  message: A client certificate used to authenticate to the apiserver is expiring
    in less than 24.0 hours.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              27.128s ago
            </td>
            <td>376.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22AggregatedAPIErrors%22%7D&amp;g0.tab=1">AggregatedAPIErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28name%2C+namespace%29+%28increase%28aggregator_unavailable_apiservice_count%5B5m%5D%29%29+%3E+2&amp;g0.tab=1">sum
  by(name, namespace) (increase(aggregator_unavailable_apiservice_count[5m])) &gt;
  2</a>
labels:
  severity: warning
annotations:
  message: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported
    errors. The number of errors have increased for it in the past five minutes. High
    values indicate that the availability of the service changes too often.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              27.127s ago
            </td>
            <td>141.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22AggregatedAPIDown%22%7D&amp;g0.tab=1">AggregatedAPIDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%281+-+max+by%28name%2C+namespace%29+%28avg_over_time%28aggregator_unavailable_apiservice%5B5m%5D%29%29%29+%2A+100+%3C+90&amp;g0.tab=1">(1
  - max by(name, namespace) (avg_over_time(aggregator_unavailable_apiservice[5m])))
  * 100 &lt; 90</a>
for: 5m
labels:
  severity: warning
annotations:
  message: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only
    {{ $value | humanize }}% available over the last 5m.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapidown
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              27.127s ago
            </td>
            <td>154.9us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeAPIDown%22%7D&amp;g0.tab=1">KubeAPIDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=absent%28up%7Bjob%3D%22apiserver%22%7D+%3D%3D+1%29&amp;g0.tab=1">absent(up{job="apiserver"}
  == 1)</a>
for: 15m
labels:
  severity: critical
annotations:
  message: KubeAPI has disappeared from Prometheus target discovery.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              27.127s ago
            </td>
            <td>92.11us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-system-controller-manager" id="kubernetes-system-controller-manager">kubernetes-system-controller-manager</a></h2></td>
            <td><h2>19.309s ago</h2></td>
            <td><h2>528.5us</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeControllerManagerDown%22%7D&amp;g0.tab=1">KubeControllerManagerDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=absent%28up%7Bjob%3D%22kube-controller-manager%22%7D+%3D%3D+1%29&amp;g0.tab=1">absent(up{job="kube-controller-manager"}
  == 1)</a>
for: 15m
labels:
  severity: critical
annotations:
  message: KubeControllerManager has disappeared from Prometheus target discovery.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              19.309s ago
            </td>
            <td>520us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-system-kubelet" id="kubernetes-system-kubelet">kubernetes-system-kubelet</a></h2></td>
            <td><h2>8.377s ago</h2></td>
            <td><h2>31.71ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeNodeNotReady%22%7D&amp;g0.tab=1">KubeNodeNotReady</a>
expr: <a href="http://localhost:9090/graph?g0.expr=kube_node_status_condition%7Bcondition%3D%22Ready%22%2Cjob%3D%22kube-state-metrics%22%2Cstatus%3D%22true%22%7D+%3D%3D+0&amp;g0.tab=1">kube_node_status_condition{condition="Ready",job="kube-state-metrics",status="true"}
  == 0</a>
for: 15m
labels:
  severity: warning
annotations:
  message: '{{ $labels.node }} has been unready for more than 15 minutes.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.377s ago
            </td>
            <td>542.3us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeNodeUnreachable%22%7D&amp;g0.tab=1">KubeNodeUnreachable</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+unless+ignoring%28key%2C+value%29+kube_node_spec_taint%7Bjob%3D%22kube-state-metrics%22%2Ckey%3D~%22ToBeDeletedByClusterAutoscaler%7Ccloud.google.com%2Fimpending-node-termination%7Caws-node-termination-handler%2Fspot-itn%22%7D%29+%3D%3D+1&amp;g0.tab=1">(kube_node_spec_taint{effect="NoSchedule",job="kube-state-metrics",key="node.kubernetes.io/unreachable"}
  unless ignoring(key, value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"})
  == 1</a>
labels:
  severity: warning
annotations:
  message: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.376s ago
            </td>
            <td>289.9us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeletTooManyPods%22%7D&amp;g0.tab=1">KubeletTooManyPods</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+by%28node%29+%28%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D%22Running%22%7D+%3D%3D+1%29+%2A+on%28instance%2C+pod%2C+namespace%2C+cluster%29+group_left%28node%29+topk+by%28instance%2C+pod%2C+namespace%2C+cluster%29+%281%2C+kube_pod_info%7Bjob%3D%22kube-state-metrics%22%7D%29%29+%2F+max+by%28node%29+%28kube_node_status_capacity_pods%7Bjob%3D%22kube-state-metrics%22%7D+%21%3D+1%29+%3E+0.95&amp;g0.tab=1">count
  by(node) ((kube_pod_status_phase{job="kube-state-metrics",phase="Running"}
  == 1) * on(instance, pod, namespace, cluster) group_left(node) topk by(instance,
  pod, namespace, cluster) (1, kube_pod_info{job="kube-state-metrics"})) /
  max by(node) (kube_node_status_capacity_pods{job="kube-state-metrics"} !=
  1) &gt; 0.95</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
    }} of its Pod capacity.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.376s ago
            </td>
            <td>21.82ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeNodeReadinessFlapping%22%7D&amp;g0.tab=1">KubeNodeReadinessFlapping</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28node%29+%28changes%28kube_node_status_condition%7Bcondition%3D%22Ready%22%2Cstatus%3D%22true%22%7D%5B15m%5D%29%29+%3E+2&amp;g0.tab=1">sum
  by(node) (changes(kube_node_status_condition{condition="Ready",status="true"}[15m]))
  &gt; 2</a>
for: 15m
labels:
  severity: warning
annotations:
  message: The readiness status of node {{ $labels.node }} has changed {{ $value }}
    times in the last 15 minutes.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodereadinessflapping
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.355s ago
            </td>
            <td>510.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeletPlegDurationHigh%22%7D&amp;g0.tab=1">KubeletPlegDurationHigh</a>
expr: <a href="http://localhost:9090/graph?g0.expr=node_quantile%3Akubelet_pleg_relist_duration_seconds%3Ahistogram_quantile%7Bquantile%3D%220.99%22%7D+%3E%3D+10&amp;g0.tab=1">node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"}
  &gt;= 10</a>
for: 5m
labels:
  severity: warning
annotations:
  message: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration
    of {{ $value }} seconds on node {{ $labels.node }}.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletplegdurationhigh
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.354s ago
            </td>
            <td>401.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeletPodStartUpLatencyHigh%22%7D&amp;g0.tab=1">KubeletPodStartUpLatencyHigh</a>
expr: <a href="http://localhost:9090/graph?g0.expr=histogram_quantile%280.99%2C+sum+by%28instance%2C+le%29+%28rate%28kubelet_pod_worker_duration_seconds_bucket%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D%5B5m%5D%29%29%29+%2A+on%28instance%29+group_left%28node%29+kubelet_node_name%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D+%3E+60&amp;g0.tab=1">histogram_quantile(0.99,
  sum by(instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet",metrics_path="/metrics"}[5m])))
  * on(instance) group_left(node) kubelet_node_name{job="kubelet",metrics_path="/metrics"}
  &gt; 60</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on
    node {{ $labels.node }}.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletpodstartuplatencyhigh
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.354s ago
            </td>
            <td>7.65ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeletDown%22%7D&amp;g0.tab=1">KubeletDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=absent%28up%7Bjob%3D%22kubelet%22%2Cmetrics_path%3D%22%2Fmetrics%22%7D+%3D%3D+1%29&amp;g0.tab=1">absent(up{job="kubelet",metrics_path="/metrics"}
  == 1)</a>
for: 15m
labels:
  severity: critical
annotations:
  message: Kubelet has disappeared from Prometheus target discovery.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.347s ago
            </td>
            <td>472.7us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-system-scheduler" id="kubernetes-system-scheduler">kubernetes-system-scheduler</a></h2></td>
            <td><h2>15.256s ago</h2></td>
            <td><h2>564.5us</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeSchedulerDown%22%7D&amp;g0.tab=1">KubeSchedulerDown</a>
expr: <a href="http://localhost:9090/graph?g0.expr=absent%28up%7Bjob%3D%22kube-scheduler%22%7D+%3D%3D+1%29&amp;g0.tab=1">absent(up{job="kube-scheduler"}
  == 1)</a>
for: 15m
labels:
  severity: critical
annotations:
  message: KubeScheduler has disappeared from Prometheus target discovery.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              15.256s ago
            </td>
            <td>535.9us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#kubernetes-system" id="kubernetes-system">kubernetes-system</a></h2></td>
            <td><h2>12.29s ago</h2></td>
            <td><h2>6.377ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeVersionMismatch%22%7D&amp;g0.tab=1">KubeVersionMismatch</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count%28count+by%28gitVersion%29+%28label_replace%28kubernetes_build_info%7Bjob%21~%22kube-dns%7Ccoredns%22%7D%2C+%22gitVersion%22%2C+%22%241%22%2C+%22gitVersion%22%2C+%22%28v%5B0-9%5D%2A.%5B0-9%5D%2A%29.%2A%22%29%29%29+%3E+1&amp;g0.tab=1">count(count
  by(gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},
  "gitVersion", "$1", "gitVersion", "(v[0-9]*.[0-9]*).*")))
  &gt; 1</a>
for: 15m
labels:
  severity: warning
annotations:
  message: There are {{ $value }} different semantic versions of Kubernetes components
    running.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              12.29s ago
            </td>
            <td>1.235ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22KubeClientErrors%22%7D&amp;g0.tab=1">KubeClientErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28sum+by%28instance%2C+job%29+%28rate%28rest_client_requests_total%7Bcode%3D~%225..%22%7D%5B5m%5D%29%29+%2F+sum+by%28instance%2C+job%29+%28rate%28rest_client_requests_total%5B5m%5D%29%29%29+%3E+0.01&amp;g0.tab=1">(sum
  by(instance, job) (rate(rest_client_requests_total{code=~"5.."}[5m])) /
  sum by(instance, job) (rate(rest_client_requests_total[5m]))) &gt; 0.01</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
    }}' is experiencing {{ $value | humanizePercentage }} errors.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              12.289s ago
            </td>
            <td>5.133ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#node-exporter.rules" id="node-exporter.rules">node-exporter.rules</a></h2></td>
            <td><h2>9.056s ago</h2></td>
            <td><h2>93.08ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_num_cpu%3Asum&amp;g0.tab=1">instance:node_num_cpu:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+without%28cpu%29+%28count+without%28mode%29+%28node_cpu_seconds_total%7Bjob%3D%22node-exporter%22%7D%29%29&amp;g0.tab=1">count
  without(cpu) (count without(mode) (node_cpu_seconds_total{job="node-exporter"}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.056s ago
            </td>
            <td>40.57ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_cpu_utilisation%3Arate1m&amp;g0.tab=1">instance:node_cpu_utilisation:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=1+-+avg+without%28cpu%2C+mode%29+%28rate%28node_cpu_seconds_total%7Bjob%3D%22node-exporter%22%2Cmode%3D%22idle%22%7D%5B1m%5D%29%29&amp;g0.tab=1">1
  - avg without(cpu, mode) (rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.015s ago
            </td>
            <td>7.705ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_load1_per_cpu%3Aratio&amp;g0.tab=1">instance:node_load1_per_cpu:ratio</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_load1%7Bjob%3D%22node-exporter%22%7D+%2F+instance%3Anode_num_cpu%3Asum%7Bjob%3D%22node-exporter%22%7D%29&amp;g0.tab=1">(node_load1{job="node-exporter"}
  / instance:node_num_cpu:sum{job="node-exporter"})</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.008s ago
            </td>
            <td>1.229ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_memory_utilisation%3Aratio&amp;g0.tab=1">instance:node_memory_utilisation:ratio</a>
expr: <a href="http://localhost:9090/graph?g0.expr=1+-+%28node_memory_MemAvailable_bytes%7Bjob%3D%22node-exporter%22%7D+%2F+node_memory_MemTotal_bytes%7Bjob%3D%22node-exporter%22%7D%29&amp;g0.tab=1">1
  - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"})</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.007s ago
            </td>
            <td>1.241ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_vmstat_pgmajfault%3Arate1m&amp;g0.tab=1">instance:node_vmstat_pgmajfault:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28node_vmstat_pgmajfault%7Bjob%3D%22node-exporter%22%7D%5B1m%5D%29&amp;g0.tab=1">rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.006s ago
            </td>
            <td>748.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance_device%3Anode_disk_io_time_seconds%3Arate1m&amp;g0.tab=1">instance_device:node_disk_io_time_seconds:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28node_disk_io_time_seconds_total%7Bdevice%3D~%22nvme.%2B%7Crbd.%2B%7Csd.%2B%7Cvd.%2B%7Cxvd.%2B%7Cdm-.%2B%7Cdasd.%2B%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29&amp;g0.tab=1">rate(node_disk_io_time_seconds_total{device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+",job="node-exporter"}[1m])</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              9.005s ago
            </td>
            <td>6.343ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance_device%3Anode_disk_io_time_weighted_seconds%3Arate1m&amp;g0.tab=1">instance_device:node_disk_io_time_weighted_seconds:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28node_disk_io_time_weighted_seconds_total%7Bdevice%3D~%22nvme.%2B%7Crbd.%2B%7Csd.%2B%7Cvd.%2B%7Cxvd.%2B%7Cdm-.%2B%7Cdasd.%2B%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29&amp;g0.tab=1">rate(node_disk_io_time_weighted_seconds_total{device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+",job="node-exporter"}[1m])</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.999s ago
            </td>
            <td>6.143ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_receive_bytes_excluding_lo%3Arate1m&amp;g0.tab=1">instance:node_network_receive_bytes_excluding_lo:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28device%29+%28rate%28node_network_receive_bytes_total%7Bdevice%21%3D%22lo%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29%29&amp;g0.tab=1">sum
  without(device) (rate(node_network_receive_bytes_total{device!="lo",job="node-exporter"}[1m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.993s ago
            </td>
            <td>8.245ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_transmit_bytes_excluding_lo%3Arate1m&amp;g0.tab=1">instance:node_network_transmit_bytes_excluding_lo:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28device%29+%28rate%28node_network_transmit_bytes_total%7Bdevice%21%3D%22lo%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29%29&amp;g0.tab=1">sum
  without(device) (rate(node_network_transmit_bytes_total{device!="lo",job="node-exporter"}[1m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.984s ago
            </td>
            <td>8.361ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_receive_drop_excluding_lo%3Arate1m&amp;g0.tab=1">instance:node_network_receive_drop_excluding_lo:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28device%29+%28rate%28node_network_receive_drop_total%7Bdevice%21%3D%22lo%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29%29&amp;g0.tab=1">sum
  without(device) (rate(node_network_receive_drop_total{device!="lo",job="node-exporter"}[1m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.976s ago
            </td>
            <td>6.291ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=instance%3Anode_network_transmit_drop_excluding_lo%3Arate1m&amp;g0.tab=1">instance:node_network_transmit_drop_excluding_lo:rate1m</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+without%28device%29+%28rate%28node_network_transmit_drop_total%7Bdevice%21%3D%22lo%22%2Cjob%3D%22node-exporter%22%7D%5B1m%5D%29%29&amp;g0.tab=1">sum
  without(device) (rate(node_network_transmit_drop_total{device!="lo",job="node-exporter"}[1m]))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              8.97s ago
            </td>
            <td>6.166ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#node-exporter" id="node-exporter">node-exporter</a></h2></td>
            <td><h2>4.187s ago</h2></td>
            <td><h2>44.34ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemSpaceFillingUp%22%7D&amp;g0.tab=1">NodeFilesystemSpaceFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_size_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+40+and+predict_linear%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D%5B6h%5D%2C+24+%2A+60+%2A+60%29+%3C+0+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}
  / node_filesystem_size_bytes{fstype!="",job="node-exporter"} * 100
  &lt; 40 and predict_linear(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}[6h],
  24 * 60 * 60) &lt; 0 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: warning
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available space left and is filling up.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
  summary: Filesystem is predicted to run out of space within the next 24 hours.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.188s ago
            </td>
            <td>6.87ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemSpaceFillingUp%22%7D&amp;g0.tab=1">NodeFilesystemSpaceFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_size_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+15+and+predict_linear%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D%5B6h%5D%2C+4+%2A+60+%2A+60%29+%3C+0+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}
  / node_filesystem_size_bytes{fstype!="",job="node-exporter"} * 100
  &lt; 15 and predict_linear(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}[6h],
  4 * 60 * 60) &lt; 0 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: critical
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available space left and is filling up fast.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
  summary: Filesystem is predicted to run out of space within the next 4 hours.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.181s ago
            </td>
            <td>6.263ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemAlmostOutOfSpace%22%7D&amp;g0.tab=1">NodeFilesystemAlmostOutOfSpace</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_size_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+5+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}
  / node_filesystem_size_bytes{fstype!="",job="node-exporter"} * 100
  &lt; 5 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: warning
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available space left.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
  summary: Filesystem has less than 5% space left.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.175s ago
            </td>
            <td>1.102ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemAlmostOutOfSpace%22%7D&amp;g0.tab=1">NodeFilesystemAlmostOutOfSpace</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_avail_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_size_bytes%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+3+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_avail_bytes{fstype!="",job="node-exporter"}
  / node_filesystem_size_bytes{fstype!="",job="node-exporter"} * 100
  &lt; 3 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: critical
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available space left.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
  summary: Filesystem has less than 3% space left.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.174s ago
            </td>
            <td>1.028ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemFilesFillingUp%22%7D&amp;g0.tab=1">NodeFilesystemFilesFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_files%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+40+and+predict_linear%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D%5B6h%5D%2C+24+%2A+60+%2A+60%29+%3C+0+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_files_free{fstype!="",job="node-exporter"}
  / node_filesystem_files{fstype!="",job="node-exporter"} * 100 &lt;
  40 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h],
  24 * 60 * 60) &lt; 0 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: warning
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available inodes left and is filling up.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
  summary: Filesystem is predicted to run out of inodes within the next 24 hours.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.173s ago
            </td>
            <td>5.967ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemFilesFillingUp%22%7D&amp;g0.tab=1">NodeFilesystemFilesFillingUp</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_files%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+20+and+predict_linear%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D%5B6h%5D%2C+4+%2A+60+%2A+60%29+%3C+0+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_files_free{fstype!="",job="node-exporter"}
  / node_filesystem_files{fstype!="",job="node-exporter"} * 100 &lt;
  20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h],
  4 * 60 * 60) &lt; 0 and node_filesystem_readonly{fstype!="",job="node-exporter"}
  == 0)</a>
for: 1h
labels:
  severity: critical
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
  summary: Filesystem is predicted to run out of inodes within the next 4 hours.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.167s ago
            </td>
            <td>5.865ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemAlmostOutOfFiles%22%7D&amp;g0.tab=1">NodeFilesystemAlmostOutOfFiles</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_files%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+5+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_files_free{fstype!="",job="node-exporter"}
  / node_filesystem_files{fstype!="",job="node-exporter"} * 100 &lt;
  5 and node_filesystem_readonly{fstype!="",job="node-exporter"} ==
  0)</a>
for: 1h
labels:
  severity: warning
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available inodes left.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
  summary: Filesystem has less than 5% inodes left.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.162s ago
            </td>
            <td>1.568ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeFilesystemAlmostOutOfFiles%22%7D&amp;g0.tab=1">NodeFilesystemAlmostOutOfFiles</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_filesystem_files_free%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2F+node_filesystem_files%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%2A+100+%3C+3+and+node_filesystem_readonly%7Bfstype%21%3D%22%22%2Cjob%3D%22node-exporter%22%7D+%3D%3D+0%29&amp;g0.tab=1">(node_filesystem_files_free{fstype!="",job="node-exporter"}
  / node_filesystem_files{fstype!="",job="node-exporter"} * 100 &lt;
  3 and node_filesystem_readonly{fstype!="",job="node-exporter"} ==
  0)</a>
for: 1h
labels:
  severity: critical
annotations:
  description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only
    {{ printf "%.2f" $value }}% available inodes left.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
  summary: Filesystem has less than 3% inodes left.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.16s ago
            </td>
            <td>1.118ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeNetworkReceiveErrs%22%7D&amp;g0.tab=1">NodeNetworkReceiveErrs</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28node_network_receive_errs_total%5B2m%5D%29+%3E+10&amp;g0.tab=1">increase(node_network_receive_errs_total[2m])
  &gt; 10</a>
for: 1h
labels:
  severity: warning
annotations:
  description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
    {{ printf "%.0f" $value }} receive errors in the last two minutes.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
  summary: Network interface is reporting many receive errors.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.159s ago
            </td>
            <td>5.731ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeNetworkTransmitErrs%22%7D&amp;g0.tab=1">NodeNetworkTransmitErrs</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28node_network_transmit_errs_total%5B2m%5D%29+%3E+10&amp;g0.tab=1">increase(node_network_transmit_errs_total[2m])
  &gt; 10</a>
for: 1h
labels:
  severity: warning
annotations:
  description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
    {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
  summary: Network interface is reporting many transmit errors.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.154s ago
            </td>
            <td>5.769ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeHighNumberConntrackEntriesUsed%22%7D&amp;g0.tab=1">NodeHighNumberConntrackEntriesUsed</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_nf_conntrack_entries+%2F+node_nf_conntrack_entries_limit%29+%3E+0.75&amp;g0.tab=1">(node_nf_conntrack_entries
  / node_nf_conntrack_entries_limit) &gt; 0.75</a>
labels:
  severity: warning
annotations:
  description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodehighnumberconntrackentriesused
  summary: Number of conntrack are getting close to the limit.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.148s ago
            </td>
            <td>735.9us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeTextFileCollectorScrapeError%22%7D&amp;g0.tab=1">NodeTextFileCollectorScrapeError</a>
expr: <a href="http://localhost:9090/graph?g0.expr=node_textfile_scrape_error%7Bjob%3D%22node-exporter%22%7D+%3D%3D+1&amp;g0.tab=1">node_textfile_scrape_error{job="node-exporter"}
  == 1</a>
labels:
  severity: warning
annotations:
  description: Node Exporter text file collector failed to scrape.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodetextfilecollectorscrapeerror
  summary: Node Exporter text file collector failed to scrape.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.147s ago
            </td>
            <td>328.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeClockSkewDetected%22%7D&amp;g0.tab=1">NodeClockSkewDetected</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28node_timex_offset_seconds+%3E+0.05+and+deriv%28node_timex_offset_seconds%5B5m%5D%29+%3E%3D+0%29+or+%28node_timex_offset_seconds+%3C+-0.05+and+deriv%28node_timex_offset_seconds%5B5m%5D%29+%3C%3D+0%29&amp;g0.tab=1">(node_timex_offset_seconds
  &gt; 0.05 and deriv(node_timex_offset_seconds[5m]) &gt;= 0) or (node_timex_offset_seconds
  &lt; -0.05 and deriv(node_timex_offset_seconds[5m]) &lt;= 0)</a>
for: 10m
labels:
  severity: warning
annotations:
  message: Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure
    NTP is configured correctly on this host.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclockskewdetected
  summary: Clock skew detected.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.147s ago
            </td>
            <td>1.624ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeClockNotSynchronising%22%7D&amp;g0.tab=1">NodeClockNotSynchronising</a>
expr: <a href="http://localhost:9090/graph?g0.expr=min_over_time%28node_timex_sync_status%5B5m%5D%29+%3D%3D+0&amp;g0.tab=1">min_over_time(node_timex_sync_status[5m])
  == 0</a>
for: 10m
labels:
  severity: warning
annotations:
  message: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured
    on this host.
  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclocknotsynchronising
  summary: Clock not synchronising.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.146s ago
            </td>
            <td>340.9us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#node-network" id="node-network">node-network</a></h2></td>
            <td><h2>4.378s ago</h2></td>
            <td><h2>7.012ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22NodeNetworkInterfaceFlapping%22%7D&amp;g0.tab=1">NodeNetworkInterfaceFlapping</a>
expr: <a href="http://localhost:9090/graph?g0.expr=changes%28node_network_up%7Bdevice%21~%22veth.%2B%22%2Cjob%3D%22node-exporter%22%7D%5B2m%5D%29+%3E+2&amp;g0.tab=1">changes(node_network_up{device!~"veth.+",job="node-exporter"}[2m])
  &gt; 2</a>
for: 2m
labels:
  severity: warning
annotations:
  message: Network interface "{{ $labels.device }}" changing it's up status
    often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              4.378s ago
            </td>
            <td>6.97ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#node.rules" id="node.rules">node.rules</a></h2></td>
            <td><h2>29.302s ago</h2></td>
            <td><h2>86.77ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=%3Akube_pod_info_node_count%3A&amp;g0.tab=1">:kube_pod_info_node_count:</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum%28min+by%28cluster%2C+node%29+%28kube_pod_info%7Bnode%21%3D%22%22%7D%29%29&amp;g0.tab=1">sum(min
  by(cluster, node) (kube_pod_info{node!=""}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              29.302s ago
            </td>
            <td>7.811ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node_namespace_pod%3Akube_pod_info%3A&amp;g0.tab=1">node_namespace_pod:kube_pod_info:</a>
expr: <a href="http://localhost:9090/graph?g0.expr=topk+by%28namespace%2C+pod%29+%281%2C+max+by%28node%2C+namespace%2C+pod%29+%28label_replace%28kube_pod_info%7Bjob%3D%22kube-state-metrics%22%2Cnode%21%3D%22%22%7D%2C+%22pod%22%2C+%22%241%22%2C+%22pod%22%2C+%22%28.%2A%29%22%29%29%29&amp;g0.tab=1">topk
  by(namespace, pod) (1, max by(node, namespace, pod) (label_replace(kube_pod_info{job="kube-state-metrics",node!=""},
  "pod", "$1", "pod", "(.*)")))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              29.294s ago
            </td>
            <td>21.1ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=node%3Anode_num_cpu%3Asum&amp;g0.tab=1">node:node_num_cpu:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=count+by%28cluster%2C+node%29+%28sum+by%28node%2C+cpu%29+%28node_cpu_seconds_total%7Bjob%3D%22node-exporter%22%7D+%2A+on%28namespace%2C+pod%29+group_left%28node%29+node_namespace_pod%3Akube_pod_info%3A%29%29&amp;g0.tab=1">count
  by(cluster, node) (sum by(node, cpu) (node_cpu_seconds_total{job="node-exporter"}
  * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              29.273s ago
            </td>
            <td>54.97ms</td>
          </tr>
          
          <tr>
            <td class="rule_cell">record: <a href="http://localhost:9090/graph?g0.expr=%3Anode_memory_MemAvailable_bytes%3Asum&amp;g0.tab=1">:node_memory_MemAvailable_bytes:sum</a>
expr: <a href="http://localhost:9090/graph?g0.expr=sum+by%28cluster%29+%28node_memory_MemAvailable_bytes%7Bjob%3D%22node-exporter%22%7D+or+%28node_memory_Buffers_bytes%7Bjob%3D%22node-exporter%22%7D+%2B+node_memory_Cached_bytes%7Bjob%3D%22node-exporter%22%7D+%2B+node_memory_MemFree_bytes%7Bjob%3D%22node-exporter%22%7D+%2B+node_memory_Slab_bytes%7Bjob%3D%22node-exporter%22%7D%29%29&amp;g0.tab=1">sum
  by(cluster) (node_memory_MemAvailable_bytes{job="node-exporter"} or (node_memory_Buffers_bytes{job="node-exporter"}
  + node_memory_Cached_bytes{job="node-exporter"} + node_memory_MemFree_bytes{job="node-exporter"}
  + node_memory_Slab_bytes{job="node-exporter"}))</a>
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              29.218s ago
            </td>
            <td>2.863ms</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#prometheus-operator" id="prometheus-operator">prometheus-operator</a></h2></td>
            <td><h2>15.756s ago</h2></td>
            <td><h2>1.032ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusOperatorListErrors%22%7D&amp;g0.tab=1">PrometheusOperatorListErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28sum+by%28controller%2C+namespace%29+%28rate%28prometheus_operator_list_operations_failed_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B1h%5D%29%29+%2F+sum+by%28controller%2C+namespace%29+%28rate%28prometheus_operator_list_operations_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B1h%5D%29%29%29+%3E+0.4&amp;g0.tab=1">(sum
  by(controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator-operator",namespace="monitoring"}[1h]))
  / sum by(controller, namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator-operator",namespace="monitoring"}[1h])))
  &gt; 0.4</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Errors while performing List operations in controller {{$labels.controller}}
    in {{$labels.namespace}} namespace.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              15.757s ago
            </td>
            <td>554.8us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusOperatorWatchErrors%22%7D&amp;g0.tab=1">PrometheusOperatorWatchErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28sum+by%28controller%2C+namespace%29+%28rate%28prometheus_operator_watch_operations_failed_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B1h%5D%29%29+%2F+sum+by%28controller%2C+namespace%29+%28rate%28prometheus_operator_watch_operations_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B1h%5D%29%29%29+%3E+0.4&amp;g0.tab=1">(sum
  by(controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator-operator",namespace="monitoring"}[1h]))
  / sum by(controller, namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator-operator",namespace="monitoring"}[1h])))
  &gt; 0.4</a>
for: 15m
labels:
  severity: warning
annotations:
  message: Errors while performing Watch operations in controller {{$labels.controller}}
    in {{$labels.namespace}} namespace.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              15.756s ago
            </td>
            <td>283.1us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusOperatorReconcileErrors%22%7D&amp;g0.tab=1">PrometheusOperatorReconcileErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28prometheus_operator_reconcile_errors_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0.1&amp;g0.tab=1">rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator-operator",namespace="monitoring"}[5m])
  &gt; 0.1</a>
for: 10m
labels:
  severity: warning
annotations:
  message: Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
    }} Namespace.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              15.756s ago
            </td>
            <td>101.7us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusOperatorNodeLookupErrors%22%7D&amp;g0.tab=1">PrometheusOperatorNodeLookupErrors</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28prometheus_operator_node_address_lookup_errors_total%7Bjob%3D%22prometheus-operator-operator%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0.1&amp;g0.tab=1">rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator-operator",namespace="monitoring"}[5m])
  &gt; 0.1</a>
for: 10m
labels:
  severity: warning
annotations:
  message: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              15.756s ago
            </td>
            <td>79.61us</td>
          </tr>
          
      
        </tbody><thead>
          <tr>
            <td colspan="3"><h2><a href="http://localhost:9090/rules#prometheus" id="prometheus">prometheus</a></h2></td>
            <td><h2>18.322s ago</h2></td>
            <td><h2>2.432ms</h2></td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="font-weight:bold">Rule</td>
            <td style="font-weight:bold">State</td>
            <td style="font-weight:bold">Error</td>
            <td style="font-weight:bold">Last Evaluation</td>
            <td style="font-weight:bold">Evaluation Time</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusBadConfig%22%7D&amp;g0.tab=1">PrometheusBadConfig</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max_over_time%28prometheus_config_last_reload_successful%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3D%3D+0&amp;g0.tab=1">max_over_time(prometheus_config_last_reload_successful{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  == 0</a>
for: 10m
labels:
  severity: critical
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload
    its configuration.
  summary: Failed Prometheus configuration reload.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>234.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusNotificationQueueRunningFull%22%7D&amp;g0.tab=1">PrometheusNotificationQueueRunningFull</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28predict_linear%28prometheus_notifications_queue_length%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%2C+60+%2A+30%29+%3E+min_over_time%28prometheus_notifications_queue_capacity%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29&amp;g0.tab=1">(predict_linear(prometheus_notifications_queue_length{job="prometheus-operator-prometheus",namespace="monitoring"}[5m],
  60 * 30) &gt; min_over_time(prometheus_notifications_queue_capacity{job="prometheus-operator-prometheus",namespace="monitoring"}[5m]))</a>
for: 15m
labels:
  severity: warning
annotations:
  description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
    is running full.
  summary: Prometheus alert notification queue predicted to run full in less than
    30m.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>254.2us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusErrorSendingAlertsToSomeAlertmanagers%22%7D&amp;g0.tab=1">PrometheusErrorSendingAlertsToSomeAlertmanagers</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28rate%28prometheus_notifications_errors_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%2F+rate%28prometheus_notifications_sent_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29+%2A+100+%3E+1&amp;g0.tab=1">(rate(prometheus_notifications_errors_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  / rate(prometheus_notifications_sent_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m]))
  * 100 &gt; 1</a>
for: 15m
labels:
  severity: warning
annotations:
  description: '{{ printf "%.1f" $value }}% errors while sending alerts from
    Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
  summary: Prometheus has encountered more than 1% errors sending alerts to a specific
    Alertmanager.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>169.7us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusErrorSendingAlertsToAnyAlertmanager%22%7D&amp;g0.tab=1">PrometheusErrorSendingAlertsToAnyAlertmanager</a>
expr: <a href="http://localhost:9090/graph?g0.expr=min+without%28alertmanager%29+%28rate%28prometheus_notifications_errors_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%2F+rate%28prometheus_notifications_sent_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29+%2A+100+%3E+3&amp;g0.tab=1">min
  without(alertmanager) (rate(prometheus_notifications_errors_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  / rate(prometheus_notifications_sent_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m]))
  * 100 &gt; 3</a>
for: 15m
labels:
  severity: critical
annotations:
  description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
    from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
  summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>161.4us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusNotConnectedToAlertmanagers%22%7D&amp;g0.tab=1">PrometheusNotConnectedToAlertmanagers</a>
expr: <a href="http://localhost:9090/graph?g0.expr=max_over_time%28prometheus_notifications_alertmanagers_discovered%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3C+1&amp;g0.tab=1">max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &lt; 1</a>
for: 10m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to
    any Alertmanagers.
  summary: Prometheus is not connected to any Alertmanagers.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>84.91us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusTSDBReloadsFailing%22%7D&amp;g0.tab=1">PrometheusTSDBReloadsFailing</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28prometheus_tsdb_reloads_failures_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B3h%5D%29+%3E+0&amp;g0.tab=1">increase(prometheus_tsdb_reloads_failures_total{job="prometheus-operator-prometheus",namespace="monitoring"}[3h])
  &gt; 0</a>
for: 4h
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
    | humanize}} reload failures over the last 3h.
  summary: Prometheus has issues reloading blocks from disk.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>272us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusTSDBCompactionsFailing%22%7D&amp;g0.tab=1">PrometheusTSDBCompactionsFailing</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28prometheus_tsdb_compactions_failed_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B3h%5D%29+%3E+0&amp;g0.tab=1">increase(prometheus_tsdb_compactions_failed_total{job="prometheus-operator-prometheus",namespace="monitoring"}[3h])
  &gt; 0</a>
for: 4h
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
    | humanize}} compaction failures over the last 3h.
  summary: Prometheus has issues compacting blocks.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>211.3us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusNotIngestingSamples%22%7D&amp;g0.tab=1">PrometheusNotIngestingSamples</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28prometheus_tsdb_head_samples_appended_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3C%3D+0&amp;g0.tab=1">rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &lt;= 0</a>
for: 10m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
  summary: Prometheus is not ingesting samples.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>84.21us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusDuplicateTimestamps%22%7D&amp;g0.tab=1">PrometheusDuplicateTimestamps</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28prometheus_target_scrapes_sample_duplicate_timestamp_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0&amp;g0.tab=1">rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &gt; 0</a>
for: 10m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf
    "%.4g" $value  }} samples/s with different values but duplicated timestamp.
  summary: Prometheus is dropping samples with duplicate timestamps.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>80.01us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusOutOfOrderTimestamps%22%7D&amp;g0.tab=1">PrometheusOutOfOrderTimestamps</a>
expr: <a href="http://localhost:9090/graph?g0.expr=rate%28prometheus_target_scrapes_sample_out_of_order_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0&amp;g0.tab=1">rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &gt; 0</a>
for: 10m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf
    "%.4g" $value  }} samples/s with timestamps arriving out of order.
  summary: Prometheus drops samples with out-of-order timestamps.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>75.7us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusRemoteStorageFailures%22%7D&amp;g0.tab=1">PrometheusRemoteStorageFailures</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28rate%28prometheus_remote_storage_failed_samples_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%2F+%28rate%28prometheus_remote_storage_failed_samples_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%2B+rate%28prometheus_remote_storage_succeeded_samples_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29%29+%2A+100+%3E+1&amp;g0.tab=1">(rate(prometheus_remote_storage_failed_samples_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  / (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  + rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])))
  * 100 &gt; 1</a>
for: 15m
labels:
  severity: critical
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{
    printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
    $labels.url }}
  summary: Prometheus fails to send samples to remote storage.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>170.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusRemoteWriteBehind%22%7D&amp;g0.tab=1">PrometheusRemoteWriteBehind</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28max_over_time%28prometheus_remote_storage_highest_timestamp_in_seconds%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+-+on%28job%2C+instance%29+group_right%28%29+max_over_time%28prometheus_remote_storage_queue_highest_sent_timestamp_seconds%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29+%3E+120&amp;g0.tab=1">(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  - on(job, instance) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-operator-prometheus",namespace="monitoring"}[5m]))
  &gt; 120</a>
for: 15m
labels:
  severity: critical
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{
    printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url
    }}.
  summary: Prometheus remote write is behind.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>132.3us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusRemoteWriteDesiredShards%22%7D&amp;g0.tab=1">PrometheusRemoteWriteDesiredShards</a>
expr: <a href="http://localhost:9090/graph?g0.expr=%28max_over_time%28prometheus_remote_storage_shards_desired%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+max_over_time%28prometheus_remote_storage_shards_max%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29%29&amp;g0.tab=1">(max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &gt; max_over_time(prometheus_remote_storage_shards_max{job="prometheus-operator-prometheus",namespace="monitoring"}[5m]))</a>
for: 15m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired
    shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{
    $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-operator-prometheus",namespace="monitoring"}`
    $labels.instance | query | first | value }}.
  summary: Prometheus remote write desired shards calculation wants to run more than
    configured max shards.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>93.61us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusRuleFailures%22%7D&amp;g0.tab=1">PrometheusRuleFailures</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28prometheus_rule_evaluation_failures_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0&amp;g0.tab=1">increase(prometheus_rule_evaluation_failures_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &gt; 0</a>
for: 15m
labels:
  severity: critical
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate
    {{ printf "%.0f" $value }} rules in the last 5m.
  summary: Prometheus is failing rule evaluations.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>312.5us</td>
          </tr>
          
          <tr>
            <td class="rule_cell">alert: <a href="http://localhost:9090/graph?g0.expr=ALERTS%7Balertname%3D%22PrometheusMissingRuleEvaluations%22%7D&amp;g0.tab=1">PrometheusMissingRuleEvaluations</a>
expr: <a href="http://localhost:9090/graph?g0.expr=increase%28prometheus_rule_group_iterations_missed_total%7Bjob%3D%22prometheus-operator-prometheus%22%2Cnamespace%3D%22monitoring%22%7D%5B5m%5D%29+%3E+0&amp;g0.tab=1">increase(prometheus_rule_group_iterations_missed_total{job="prometheus-operator-prometheus",namespace="monitoring"}[5m])
  &gt; 0</a>
for: 15m
labels:
  severity: warning
annotations:
  description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf
    "%.0f" $value }} rule group evaluations in the last 5m.
  summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
</td>
            <td class="state">
              <span class="alert alert-success state_indicator text-uppercase">
                ok
              </span>
            </td>
            <td class="errors">
              
            </td>
            <td>
              18.322s ago
            </td>
            <td>76.8us</td>
          </tr>
          
      
      </tbody>
    </table>
  </div>

    




</body></html>